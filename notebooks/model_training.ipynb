{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs the following key tasks:\n",
    "\n",
    "1. **Load the Dataset**:  \n",
    "   - Reads a `.parquet` file into a pandas DataFrame using the `pyarrow` engine.\n",
    "\n",
    "2. **Remap Labels**:  \n",
    "   - The `label_mapping` dictionary is used to map sentiment labels from their original values (e.g., 0, 1, 3, 4) to a continuous range (0, 1, 2, 3).  \n",
    "   - This transformation simplifies label interpretation and ensures compatibility with models expecting sequential integers.\n",
    "\n",
    "3. **Train-Test Split**:  \n",
    "   - Splits the dataset into training and testing sets with an 80-20 ratio.\n",
    "   - Separates the `text` (features) and `label` (targets) columns using `train_test_split`.\n",
    "\n",
    "4. **Verify Remapped Labels**:  \n",
    "   - Prints the unique remapped labels to confirm the transformation.\n",
    "\n",
    "### Purpose:\n",
    "- Prepares the dataset for training and evaluation by splitting it into train/test sets and ensuring labels are properly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKcshEkMKe7f",
    "outputId": "d2cb1a83-9042-4160-b5c0-8c41bf155bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels after remapping: [1 3 0 2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_parquet(r\"C:\\Users\\saall\\Desktop\\Arabic Sentiment Analysis for Hotel Reviews Multi-Class Prediction Model\\data\\train-00000-of-00001.parquet\", engine=\"pyarrow\")\n",
    "# Remap labels to a continuous range\n",
    "label_mapping = {0: 0, 1: 1, 3: 2, 4: 3}\n",
    "df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Labels after remapping:\", df['label'].unique()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Define `preprocess_text` Function**:  \n",
    "   - Removes any leading or trailing whitespace from a given text string using Python's built-in `.strip()` method.  \n",
    "   - Helps clean the data by ensuring consistency in the text formatting.\n",
    "\n",
    "2. **Apply the Function to the Dataset**:  \n",
    "   - Uses the pandas `.apply()` method to apply the `preprocess_text` function to each entry in the `text` column of the DataFrame.  \n",
    "   - Updates the `text` column in place with the cleaned version of each text entry.\n",
    "\n",
    "### Purpose:  \n",
    "This step ensures that the text data is free of unnecessary whitespace, which can be crucial for improving the quality of tokenization and subsequent NLP model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-rU0qgbK-rn"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Strip extra whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Purpose of `train_test_split`**:  \n",
    "   - Splits the dataset into training and testing subsets for supervised machine learning tasks.  \n",
    "   - Ensures that the model has separate data for training (to learn patterns) and testing (to evaluate performance).\n",
    "\n",
    "2. **Parameters**:  \n",
    "   - `df['text']`: The text data to be used as input features.\n",
    "   - `df['label']`: The target labels corresponding to each text entry.\n",
    "   - `test_size=0.2`: Reserves 20% of the data for testing and 80% for training.\n",
    "   - `random_state=42`: Ensures the split is reproducible, providing consistent results across runs.\n",
    "\n",
    "3. **Outputs**:  \n",
    "   - `train_texts`: Text data for training the model.\n",
    "   - `test_texts`: Text data for evaluating the model.\n",
    "   - `train_labels`: Corresponding labels for the training data.\n",
    "   - `test_labels`: Corresponding labels for the testing data.\n",
    "\n",
    "### Purpose:  \n",
    "This step ensures a fair split of the data into training and testing subsets, enabling the model to generalize better and preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWesFQCaLBAZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Purpose of `AutoTokenizer`:**  \n",
    "   - Automatically loads the tokenizer compatible with the specified model (`AraBERT` in this case).  \n",
    "   - `AraBERT` is designed to handle Arabic text preprocessing and tokenization efficiently.\n",
    "\n",
    "2. **Parameters for Tokenizer**:  \n",
    "   - `list(train_texts)` and `list(test_texts)`: Converts training and testing text data into lists for processing.  \n",
    "   - `truncation=True`: Ensures that texts longer than the `max_length` are truncated to prevent overflow.  \n",
    "   - `padding=True`: Adds padding to ensure that all sequences in a batch have the same length, required for model compatibility.  \n",
    "   - `max_length=128`: Limits the tokenized sequence length to 128 tokens.\n",
    "\n",
    "3. **Outputs**:  \n",
    "   - `train_encodings`: Dictionary containing tokenized training data with keys like `input_ids` and `attention_mask`.  \n",
    "   - `test_encodings`: Dictionary containing tokenized test data, structured similarly to training data.\n",
    "\n",
    "### Purpose:  \n",
    "This code prepares the text data for input into the `AraBERT` model by converting raw Arabic text into tokenized sequences that the model can process. Tokenization handles splitting text into tokens, padding, and truncating, ensuring uniformity for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elG50dvPLDkE",
    "outputId": "7fb264f2-24d8-44ae-dd75-0a1ba8de26e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load AraBERT tokenizer\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Purpose of `SentimentDataset` Class**:  \n",
    "   - Custom PyTorch `Dataset` implementation to handle tokenized data and labels.  \n",
    "   - Converts tokenized data (from the tokenizer) into a format compatible with PyTorch models.\n",
    "\n",
    "2. **Key Methods**:  \n",
    "   - `__init__`:  \n",
    "     - Accepts `encodings` (tokenized input data) and `labels` (sentiment labels).  \n",
    "     - Stores them as instance variables for further access.  \n",
    "   - `__len__`:  \n",
    "     - Returns the length of the dataset, equivalent to the number of labels.  \n",
    "   - `__getitem__`:  \n",
    "     - Retrieves a specific data point by index (`idx`).  \n",
    "     - Converts each part of the tokenized data and its corresponding label into PyTorch tensors.  \n",
    "     - Returns a dictionary containing the tokenized inputs (`input_ids`, `attention_mask`, etc.) and the label.\n",
    "\n",
    "3. **Dataset Creation**:  \n",
    "   - `train_dataset` and `test_dataset` instances are created using `train_encodings` and `test_encodings` (from tokenization) along with their corresponding labels.\n",
    "\n",
    "### Purpose:  \n",
    "The `SentimentDataset` class and its instances (`train_dataset` and `test_dataset`) structure the data for PyTorch's DataLoader. This enables efficient batch processing, shuffling, and iteration during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxtwFwaeLIsK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = SentimentDataset(train_encodings, list(train_labels))\n",
    "test_dataset = SentimentDataset(test_encodings, list(test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading AraBERT Model for Sequence Classification\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "```\n",
    "- **Importing `AutoModelForSequenceClassification`**:  \n",
    "  This imports the `AutoModelForSequenceClassification` class from the Hugging Face `transformers` library. This class provides a convenient way to load pre-trained models for sequence classification tasks, such as sentiment analysis.\n",
    "\n",
    "```python\n",
    "# Load AraBERT with 4 output labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "```\n",
    "- **Loading the Pre-Trained AraBERT Model**:  \n",
    "  - **`AutoModelForSequenceClassification`**: This class automatically selects and loads a suitable pre-trained model for sequence classification tasks (like sentiment analysis).\n",
    "  - **`from_pretrained(model_name)`**: This method loads a pre-trained model based on the name provided. In this case, `model_name` is set to `\"aubmindlab/bert-base-arabertv02\"`, which refers to a pre-trained Arabic BERT model by the AUB Mind Lab.\n",
    "  - **`num_labels=4`**: This specifies that the model should be configured for a classification task with 4 possible output labels (0, 1, 3, and 4 in this case). This is important because the pre-trained BERT model was originally designed for a different number of labels, and setting `num_labels` ensures the output layer of the model is configured accordingly.\n",
    "\n",
    "### Summary:\n",
    "- The code loads the pre-trained AraBERT model for sequence classification, and it specifies that the model will have 4 possible output labels. This setup is essential for training the model to predict sentiment labels from the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pxy3Yov7Lapf",
    "outputId": "1f1c76da-8be6-4421-9588-678a7afa7054"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load AraBERT with 4 output labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Loading**:  \n",
    "   - `DataLoader` is used to iterate through `train_dataset` and `test_dataset` in batches, enabling efficient memory use and faster training.  \n",
    "   - `train_loader`: Batches are shuffled for randomness.  \n",
    "   - `test_loader`: Shuffling isn't necessary for evaluation.  \n",
    "\n",
    "2. **Optimizer Setup**:  \n",
    "   - `AdamW`: A weight decay-optimized Adam optimizer suitable for Transformer models.  \n",
    "   - Learning rate: Set to `5e-5`, which is a typical starting point for fine-tuning large language models.  \n",
    "\n",
    "3. **Device Configuration**:  \n",
    "   - Checks for GPU availability (`torch.cuda.is_available()`), defaulting to CPU if none is available.  \n",
    "   - Model is moved to the selected device for faster computation on compatible hardware.  \n",
    "\n",
    "4. **Training Loop**:  \n",
    "   - **Outer Loop (Epochs)**: Iterates through the dataset multiple times to improve performance.  \n",
    "   - **Inner Loop (Batches)**: Processes one batch at a time:  \n",
    "     - Clears gradients with `optimizer.zero_grad()`.  \n",
    "     - Moves data (`input_ids`, `attention_mask`, and `labels`) to the device.  \n",
    "     - Performs a forward pass to compute predictions and calculate the loss.  \n",
    "     - Computes gradients via backpropagation (`loss.backward()`) and updates model weights (`optimizer.step()`).  \n",
    "   - Tracks training progress and calculates metrics like **loss** and **accuracy** after each epoch:  \n",
    "     - **Loss**: Measures model error. Lower loss indicates better performance.  \n",
    "     - **Accuracy**: Tracks the percentage of correct predictions.  \n",
    "\n",
    "5. **Metrics Display**:  \n",
    "   - Prints batch-level loss every 10 batches for progress monitoring.  \n",
    "   - Outputs average loss and accuracy at the end of each epoch for performance tracking.  \n",
    "\n",
    "### Purpose:  \n",
    "The code fine-tunes a pre-trained Transformer model (e.g., AraBERT) on the dataset by iteratively adjusting model weights to minimize prediction errors, using batches of training data for efficient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fN5MtEIRLdFd",
    "outputId": "ad365403-0f58-4e5b-bd7b-522b54579b4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10/5285, Loss: 1.2008\n",
      "Epoch 1, Batch 20/5285, Loss: 1.2568\n",
      "Epoch 1, Batch 30/5285, Loss: 1.1756\n",
      "Epoch 1, Batch 40/5285, Loss: 0.8731\n",
      "Epoch 1, Batch 50/5285, Loss: 1.1242\n",
      "Epoch 1, Batch 60/5285, Loss: 0.3804\n",
      "Epoch 1, Batch 70/5285, Loss: 1.0238\n",
      "Epoch 1, Batch 80/5285, Loss: 0.4465\n",
      "Epoch 1, Batch 90/5285, Loss: 0.6120\n",
      "Epoch 1, Batch 100/5285, Loss: 0.4893\n",
      "Epoch 1, Batch 110/5285, Loss: 0.6490\n",
      "Epoch 1, Batch 120/5285, Loss: 0.5577\n",
      "Epoch 1, Batch 130/5285, Loss: 0.6667\n",
      "Epoch 1, Batch 140/5285, Loss: 0.7095\n",
      "Epoch 1, Batch 150/5285, Loss: 0.6321\n",
      "Epoch 1, Batch 160/5285, Loss: 0.4536\n",
      "Epoch 1, Batch 170/5285, Loss: 0.2634\n",
      "Epoch 1, Batch 180/5285, Loss: 0.3555\n",
      "Epoch 1, Batch 190/5285, Loss: 0.6381\n",
      "Epoch 1, Batch 200/5285, Loss: 0.7491\n",
      "Epoch 1, Batch 210/5285, Loss: 0.6537\n",
      "Epoch 1, Batch 220/5285, Loss: 0.6732\n",
      "Epoch 1, Batch 230/5285, Loss: 0.5858\n",
      "Epoch 1, Batch 240/5285, Loss: 0.5510\n",
      "Epoch 1, Batch 250/5285, Loss: 0.3905\n",
      "Epoch 1, Batch 260/5285, Loss: 0.4549\n",
      "Epoch 1, Batch 270/5285, Loss: 0.5070\n",
      "Epoch 1, Batch 280/5285, Loss: 0.4511\n",
      "Epoch 1, Batch 290/5285, Loss: 0.2739\n",
      "Epoch 1, Batch 300/5285, Loss: 0.8612\n",
      "Epoch 1, Batch 310/5285, Loss: 0.6047\n",
      "Epoch 1, Batch 320/5285, Loss: 0.4561\n",
      "Epoch 1, Batch 330/5285, Loss: 0.7047\n",
      "Epoch 1, Batch 340/5285, Loss: 0.2291\n",
      "Epoch 1, Batch 350/5285, Loss: 0.3331\n",
      "Epoch 1, Batch 360/5285, Loss: 0.4842\n",
      "Epoch 1, Batch 370/5285, Loss: 0.6025\n",
      "Epoch 1, Batch 380/5285, Loss: 0.5136\n",
      "Epoch 1, Batch 390/5285, Loss: 0.4778\n",
      "Epoch 1, Batch 400/5285, Loss: 0.3387\n",
      "Epoch 1, Batch 410/5285, Loss: 0.4184\n",
      "Epoch 1, Batch 420/5285, Loss: 0.5409\n",
      "Epoch 1, Batch 430/5285, Loss: 0.3824\n",
      "Epoch 1, Batch 440/5285, Loss: 0.5131\n",
      "Epoch 1, Batch 450/5285, Loss: 0.4367\n",
      "Epoch 1, Batch 460/5285, Loss: 0.2327\n",
      "Epoch 1, Batch 470/5285, Loss: 0.5595\n",
      "Epoch 1, Batch 480/5285, Loss: 0.4852\n",
      "Epoch 1, Batch 490/5285, Loss: 0.4841\n",
      "Epoch 1, Batch 500/5285, Loss: 0.5384\n",
      "Epoch 1, Batch 510/5285, Loss: 0.2561\n",
      "Epoch 1, Batch 520/5285, Loss: 0.4085\n",
      "Epoch 1, Batch 530/5285, Loss: 0.3958\n",
      "Epoch 1, Batch 540/5285, Loss: 0.6700\n",
      "Epoch 1, Batch 550/5285, Loss: 0.3447\n",
      "Epoch 1, Batch 560/5285, Loss: 0.6797\n",
      "Epoch 1, Batch 570/5285, Loss: 0.3934\n",
      "Epoch 1, Batch 580/5285, Loss: 0.4522\n",
      "Epoch 1, Batch 590/5285, Loss: 0.5202\n",
      "Epoch 1, Batch 600/5285, Loss: 0.3772\n",
      "Epoch 1, Batch 610/5285, Loss: 0.2677\n",
      "Epoch 1, Batch 620/5285, Loss: 0.3186\n",
      "Epoch 1, Batch 630/5285, Loss: 0.4836\n",
      "Epoch 1, Batch 640/5285, Loss: 0.6455\n",
      "Epoch 1, Batch 650/5285, Loss: 0.5447\n",
      "Epoch 1, Batch 660/5285, Loss: 0.3343\n",
      "Epoch 1, Batch 670/5285, Loss: 0.4607\n",
      "Epoch 1, Batch 680/5285, Loss: 0.4530\n",
      "Epoch 1, Batch 690/5285, Loss: 0.4022\n",
      "Epoch 1, Batch 700/5285, Loss: 0.5742\n",
      "Epoch 1, Batch 710/5285, Loss: 0.6524\n",
      "Epoch 1, Batch 720/5285, Loss: 0.3579\n",
      "Epoch 1, Batch 730/5285, Loss: 0.3469\n",
      "Epoch 1, Batch 740/5285, Loss: 0.2589\n",
      "Epoch 1, Batch 750/5285, Loss: 0.8589\n",
      "Epoch 1, Batch 760/5285, Loss: 0.6104\n",
      "Epoch 1, Batch 770/5285, Loss: 0.1902\n",
      "Epoch 1, Batch 780/5285, Loss: 0.5653\n",
      "Epoch 1, Batch 790/5285, Loss: 0.3289\n",
      "Epoch 1, Batch 800/5285, Loss: 0.3111\n",
      "Epoch 1, Batch 810/5285, Loss: 0.3018\n",
      "Epoch 1, Batch 820/5285, Loss: 0.4805\n",
      "Epoch 1, Batch 830/5285, Loss: 0.7733\n",
      "Epoch 1, Batch 840/5285, Loss: 0.2417\n",
      "Epoch 1, Batch 850/5285, Loss: 0.2582\n",
      "Epoch 1, Batch 860/5285, Loss: 0.5685\n",
      "Epoch 1, Batch 870/5285, Loss: 0.4761\n",
      "Epoch 1, Batch 880/5285, Loss: 0.6681\n",
      "Epoch 1, Batch 890/5285, Loss: 0.4671\n",
      "Epoch 1, Batch 900/5285, Loss: 0.2474\n",
      "Epoch 1, Batch 910/5285, Loss: 0.2811\n",
      "Epoch 1, Batch 920/5285, Loss: 0.2536\n",
      "Epoch 1, Batch 930/5285, Loss: 0.2135\n",
      "Epoch 1, Batch 940/5285, Loss: 0.4292\n",
      "Epoch 1, Batch 950/5285, Loss: 0.3225\n",
      "Epoch 1, Batch 960/5285, Loss: 0.4644\n",
      "Epoch 1, Batch 970/5285, Loss: 0.2653\n",
      "Epoch 1, Batch 980/5285, Loss: 0.3563\n",
      "Epoch 1, Batch 990/5285, Loss: 0.3383\n",
      "Epoch 1, Batch 1000/5285, Loss: 0.4574\n",
      "Epoch 1, Batch 1010/5285, Loss: 0.4231\n",
      "Epoch 1, Batch 1020/5285, Loss: 0.2594\n",
      "Epoch 1, Batch 1030/5285, Loss: 0.1219\n",
      "Epoch 1, Batch 1040/5285, Loss: 0.3260\n",
      "Epoch 1, Batch 1050/5285, Loss: 0.8380\n",
      "Epoch 1, Batch 1060/5285, Loss: 0.4381\n",
      "Epoch 1, Batch 1070/5285, Loss: 0.3311\n",
      "Epoch 1, Batch 1080/5285, Loss: 0.5729\n",
      "Epoch 1, Batch 1090/5285, Loss: 0.8414\n",
      "Epoch 1, Batch 1100/5285, Loss: 0.4375\n",
      "Epoch 1, Batch 1110/5285, Loss: 0.4315\n",
      "Epoch 1, Batch 1120/5285, Loss: 0.8086\n",
      "Epoch 1, Batch 1130/5285, Loss: 0.5517\n",
      "Epoch 1, Batch 1140/5285, Loss: 0.3492\n",
      "Epoch 1, Batch 1150/5285, Loss: 0.3854\n",
      "Epoch 1, Batch 1160/5285, Loss: 0.6312\n",
      "Epoch 1, Batch 1170/5285, Loss: 0.4510\n",
      "Epoch 1, Batch 1180/5285, Loss: 0.3237\n",
      "Epoch 1, Batch 1190/5285, Loss: 0.1905\n",
      "Epoch 1, Batch 1200/5285, Loss: 0.6162\n",
      "Epoch 1, Batch 1210/5285, Loss: 0.5372\n",
      "Epoch 1, Batch 1220/5285, Loss: 0.6287\n",
      "Epoch 1, Batch 1230/5285, Loss: 0.4514\n",
      "Epoch 1, Batch 1240/5285, Loss: 0.2754\n",
      "Epoch 1, Batch 1250/5285, Loss: 0.5162\n",
      "Epoch 1, Batch 1260/5285, Loss: 0.3735\n",
      "Epoch 1, Batch 1270/5285, Loss: 0.3283\n",
      "Epoch 1, Batch 1280/5285, Loss: 0.4776\n",
      "Epoch 1, Batch 1290/5285, Loss: 0.8231\n",
      "Epoch 1, Batch 1300/5285, Loss: 0.6687\n",
      "Epoch 1, Batch 1310/5285, Loss: 0.6211\n",
      "Epoch 1, Batch 1320/5285, Loss: 0.2842\n",
      "Epoch 1, Batch 1330/5285, Loss: 0.3399\n",
      "Epoch 1, Batch 1340/5285, Loss: 0.2493\n",
      "Epoch 1, Batch 1350/5285, Loss: 0.4649\n",
      "Epoch 1, Batch 1360/5285, Loss: 0.4113\n",
      "Epoch 1, Batch 1370/5285, Loss: 0.3849\n",
      "Epoch 1, Batch 1380/5285, Loss: 0.4956\n",
      "Epoch 1, Batch 1390/5285, Loss: 0.5406\n",
      "Epoch 1, Batch 1400/5285, Loss: 0.4504\n",
      "Epoch 1, Batch 1410/5285, Loss: 1.0021\n",
      "Epoch 1, Batch 1420/5285, Loss: 0.4845\n",
      "Epoch 1, Batch 1430/5285, Loss: 0.4976\n",
      "Epoch 1, Batch 1440/5285, Loss: 0.4515\n",
      "Epoch 1, Batch 1450/5285, Loss: 0.4163\n",
      "Epoch 1, Batch 1460/5285, Loss: 0.6871\n",
      "Epoch 1, Batch 1470/5285, Loss: 0.4854\n",
      "Epoch 1, Batch 1480/5285, Loss: 0.2597\n",
      "Epoch 1, Batch 1490/5285, Loss: 0.3415\n",
      "Epoch 1, Batch 1500/5285, Loss: 0.3290\n",
      "Epoch 1, Batch 1510/5285, Loss: 0.3210\n",
      "Epoch 1, Batch 1520/5285, Loss: 0.3014\n",
      "Epoch 1, Batch 1530/5285, Loss: 0.2658\n",
      "Epoch 1, Batch 1540/5285, Loss: 1.0181\n",
      "Epoch 1, Batch 1550/5285, Loss: 0.3017\n",
      "Epoch 1, Batch 1560/5285, Loss: 0.6383\n",
      "Epoch 1, Batch 1570/5285, Loss: 0.4074\n",
      "Epoch 1, Batch 1580/5285, Loss: 0.5725\n",
      "Epoch 1, Batch 1590/5285, Loss: 0.0986\n",
      "Epoch 1, Batch 1600/5285, Loss: 0.3120\n",
      "Epoch 1, Batch 1610/5285, Loss: 0.3769\n",
      "Epoch 1, Batch 1620/5285, Loss: 0.4801\n",
      "Epoch 1, Batch 1630/5285, Loss: 0.3454\n",
      "Epoch 1, Batch 1640/5285, Loss: 0.2419\n",
      "Epoch 1, Batch 1650/5285, Loss: 0.2361\n",
      "Epoch 1, Batch 1660/5285, Loss: 0.7752\n",
      "Epoch 1, Batch 1670/5285, Loss: 0.3011\n",
      "Epoch 1, Batch 1680/5285, Loss: 0.4387\n",
      "Epoch 1, Batch 1690/5285, Loss: 0.5701\n",
      "Epoch 1, Batch 1700/5285, Loss: 0.5600\n",
      "Epoch 1, Batch 1710/5285, Loss: 0.6103\n",
      "Epoch 1, Batch 1720/5285, Loss: 0.3299\n",
      "Epoch 1, Batch 1730/5285, Loss: 0.3772\n",
      "Epoch 1, Batch 1740/5285, Loss: 0.6084\n",
      "Epoch 1, Batch 1750/5285, Loss: 0.3201\n",
      "Epoch 1, Batch 1760/5285, Loss: 0.3031\n",
      "Epoch 1, Batch 1770/5285, Loss: 0.3099\n",
      "Epoch 1, Batch 1780/5285, Loss: 0.3038\n",
      "Epoch 1, Batch 1790/5285, Loss: 0.5128\n",
      "Epoch 1, Batch 1800/5285, Loss: 0.4542\n",
      "Epoch 1, Batch 1810/5285, Loss: 0.3849\n",
      "Epoch 1, Batch 1820/5285, Loss: 0.3245\n",
      "Epoch 1, Batch 1830/5285, Loss: 0.4229\n",
      "Epoch 1, Batch 1840/5285, Loss: 0.3587\n",
      "Epoch 1, Batch 1850/5285, Loss: 0.3694\n",
      "Epoch 1, Batch 1860/5285, Loss: 0.4982\n",
      "Epoch 1, Batch 1870/5285, Loss: 0.5453\n",
      "Epoch 1, Batch 1880/5285, Loss: 0.1459\n",
      "Epoch 1, Batch 1890/5285, Loss: 0.4398\n",
      "Epoch 1, Batch 1900/5285, Loss: 0.3774\n",
      "Epoch 1, Batch 1910/5285, Loss: 0.2354\n",
      "Epoch 1, Batch 1920/5285, Loss: 0.3785\n",
      "Epoch 1, Batch 1930/5285, Loss: 0.5410\n",
      "Epoch 1, Batch 1940/5285, Loss: 0.3915\n",
      "Epoch 1, Batch 1950/5285, Loss: 0.4272\n",
      "Epoch 1, Batch 1960/5285, Loss: 0.1057\n",
      "Epoch 1, Batch 1970/5285, Loss: 0.3188\n",
      "Epoch 1, Batch 1980/5285, Loss: 0.4266\n",
      "Epoch 1, Batch 1990/5285, Loss: 0.3167\n",
      "Epoch 1, Batch 2000/5285, Loss: 0.3778\n",
      "Epoch 1, Batch 2010/5285, Loss: 0.4858\n",
      "Epoch 1, Batch 2020/5285, Loss: 0.2782\n",
      "Epoch 1, Batch 2030/5285, Loss: 0.2765\n",
      "Epoch 1, Batch 2040/5285, Loss: 0.4850\n",
      "Epoch 1, Batch 2050/5285, Loss: 0.5336\n",
      "Epoch 1, Batch 2060/5285, Loss: 0.3103\n",
      "Epoch 1, Batch 2070/5285, Loss: 0.5730\n",
      "Epoch 1, Batch 2080/5285, Loss: 0.6647\n",
      "Epoch 1, Batch 2090/5285, Loss: 0.4487\n",
      "Epoch 1, Batch 2100/5285, Loss: 0.5184\n",
      "Epoch 1, Batch 2110/5285, Loss: 0.3578\n",
      "Epoch 1, Batch 2120/5285, Loss: 0.1797\n",
      "Epoch 1, Batch 2130/5285, Loss: 0.4028\n",
      "Epoch 1, Batch 2140/5285, Loss: 0.3784\n",
      "Epoch 1, Batch 2150/5285, Loss: 0.8386\n",
      "Epoch 1, Batch 2160/5285, Loss: 0.4603\n",
      "Epoch 1, Batch 2170/5285, Loss: 0.4084\n",
      "Epoch 1, Batch 2180/5285, Loss: 0.3296\n",
      "Epoch 1, Batch 2190/5285, Loss: 0.0853\n",
      "Epoch 1, Batch 2200/5285, Loss: 0.5819\n",
      "Epoch 1, Batch 2210/5285, Loss: 0.4984\n",
      "Epoch 1, Batch 2220/5285, Loss: 0.4441\n",
      "Epoch 1, Batch 2230/5285, Loss: 0.3474\n",
      "Epoch 1, Batch 2240/5285, Loss: 0.2510\n",
      "Epoch 1, Batch 2250/5285, Loss: 0.3253\n",
      "Epoch 1, Batch 2260/5285, Loss: 0.2490\n",
      "Epoch 1, Batch 2270/5285, Loss: 0.4173\n",
      "Epoch 1, Batch 2280/5285, Loss: 0.2964\n",
      "Epoch 1, Batch 2290/5285, Loss: 0.3598\n",
      "Epoch 1, Batch 2300/5285, Loss: 0.6953\n",
      "Epoch 1, Batch 2310/5285, Loss: 0.4218\n",
      "Epoch 1, Batch 2320/5285, Loss: 0.2290\n",
      "Epoch 1, Batch 2330/5285, Loss: 0.6772\n",
      "Epoch 1, Batch 2340/5285, Loss: 0.3339\n",
      "Epoch 1, Batch 2350/5285, Loss: 0.3550\n",
      "Epoch 1, Batch 2360/5285, Loss: 0.7776\n",
      "Epoch 1, Batch 2370/5285, Loss: 0.7799\n",
      "Epoch 1, Batch 2380/5285, Loss: 0.2099\n",
      "Epoch 1, Batch 2390/5285, Loss: 0.3993\n",
      "Epoch 1, Batch 2400/5285, Loss: 0.2962\n",
      "Epoch 1, Batch 2410/5285, Loss: 0.2057\n",
      "Epoch 1, Batch 2420/5285, Loss: 0.3821\n",
      "Epoch 1, Batch 2430/5285, Loss: 0.3430\n",
      "Epoch 1, Batch 2440/5285, Loss: 0.5176\n",
      "Epoch 1, Batch 2450/5285, Loss: 0.3800\n",
      "Epoch 1, Batch 2460/5285, Loss: 0.3019\n",
      "Epoch 1, Batch 2470/5285, Loss: 0.3567\n",
      "Epoch 1, Batch 2480/5285, Loss: 0.3509\n",
      "Epoch 1, Batch 2490/5285, Loss: 0.6223\n",
      "Epoch 1, Batch 2500/5285, Loss: 0.6088\n",
      "Epoch 1, Batch 2510/5285, Loss: 0.4171\n",
      "Epoch 1, Batch 2520/5285, Loss: 0.2944\n",
      "Epoch 1, Batch 2530/5285, Loss: 0.2271\n",
      "Epoch 1, Batch 2540/5285, Loss: 0.5342\n",
      "Epoch 1, Batch 2550/5285, Loss: 0.6383\n",
      "Epoch 1, Batch 2560/5285, Loss: 0.4766\n",
      "Epoch 1, Batch 2570/5285, Loss: 0.5228\n",
      "Epoch 1, Batch 2580/5285, Loss: 0.2807\n",
      "Epoch 1, Batch 2590/5285, Loss: 0.3966\n",
      "Epoch 1, Batch 2600/5285, Loss: 0.4575\n",
      "Epoch 1, Batch 2610/5285, Loss: 0.3664\n",
      "Epoch 1, Batch 2620/5285, Loss: 0.3466\n",
      "Epoch 1, Batch 2630/5285, Loss: 0.2121\n",
      "Epoch 1, Batch 2640/5285, Loss: 0.4553\n",
      "Epoch 1, Batch 2650/5285, Loss: 0.2853\n",
      "Epoch 1, Batch 2660/5285, Loss: 0.2588\n",
      "Epoch 1, Batch 2670/5285, Loss: 0.2588\n",
      "Epoch 1, Batch 2680/5285, Loss: 0.6906\n",
      "Epoch 1, Batch 2690/5285, Loss: 0.5139\n",
      "Epoch 1, Batch 2700/5285, Loss: 0.2229\n",
      "Epoch 1, Batch 2710/5285, Loss: 0.2546\n",
      "Epoch 1, Batch 2720/5285, Loss: 0.2816\n",
      "Epoch 1, Batch 2730/5285, Loss: 0.3446\n",
      "Epoch 1, Batch 2740/5285, Loss: 0.4122\n",
      "Epoch 1, Batch 2750/5285, Loss: 0.4510\n",
      "Epoch 1, Batch 2760/5285, Loss: 0.6725\n",
      "Epoch 1, Batch 2770/5285, Loss: 0.2897\n",
      "Epoch 1, Batch 2780/5285, Loss: 0.4015\n",
      "Epoch 1, Batch 2790/5285, Loss: 0.4057\n",
      "Epoch 1, Batch 2800/5285, Loss: 0.3107\n",
      "Epoch 1, Batch 2810/5285, Loss: 0.4083\n",
      "Epoch 1, Batch 2820/5285, Loss: 0.5240\n",
      "Epoch 1, Batch 2830/5285, Loss: 0.3302\n",
      "Epoch 1, Batch 2840/5285, Loss: 0.2386\n",
      "Epoch 1, Batch 2850/5285, Loss: 0.4450\n",
      "Epoch 1, Batch 2860/5285, Loss: 0.4127\n",
      "Epoch 1, Batch 2870/5285, Loss: 0.5214\n",
      "Epoch 1, Batch 2880/5285, Loss: 0.8839\n",
      "Epoch 1, Batch 2890/5285, Loss: 0.6096\n",
      "Epoch 1, Batch 2900/5285, Loss: 0.7335\n",
      "Epoch 1, Batch 2910/5285, Loss: 0.3805\n",
      "Epoch 1, Batch 2920/5285, Loss: 0.6849\n",
      "Epoch 1, Batch 2930/5285, Loss: 0.3902\n",
      "Epoch 1, Batch 2940/5285, Loss: 0.3776\n",
      "Epoch 1, Batch 2950/5285, Loss: 0.4866\n",
      "Epoch 1, Batch 2960/5285, Loss: 0.2548\n",
      "Epoch 1, Batch 2970/5285, Loss: 0.7279\n",
      "Epoch 1, Batch 2980/5285, Loss: 0.1757\n",
      "Epoch 1, Batch 2990/5285, Loss: 0.5661\n",
      "Epoch 1, Batch 3000/5285, Loss: 0.2554\n",
      "Epoch 1, Batch 3010/5285, Loss: 0.3054\n",
      "Epoch 1, Batch 3020/5285, Loss: 0.2101\n",
      "Epoch 1, Batch 3030/5285, Loss: 0.3238\n",
      "Epoch 1, Batch 3040/5285, Loss: 0.6281\n",
      "Epoch 1, Batch 3050/5285, Loss: 0.5293\n",
      "Epoch 1, Batch 3060/5285, Loss: 0.2007\n",
      "Epoch 1, Batch 3070/5285, Loss: 0.3945\n",
      "Epoch 1, Batch 3080/5285, Loss: 0.2923\n",
      "Epoch 1, Batch 3090/5285, Loss: 0.4729\n",
      "Epoch 1, Batch 3100/5285, Loss: 0.2358\n",
      "Epoch 1, Batch 3110/5285, Loss: 0.3229\n",
      "Epoch 1, Batch 3120/5285, Loss: 0.3828\n",
      "Epoch 1, Batch 3130/5285, Loss: 0.6007\n",
      "Epoch 1, Batch 3140/5285, Loss: 0.3141\n",
      "Epoch 1, Batch 3150/5285, Loss: 0.3617\n",
      "Epoch 1, Batch 3160/5285, Loss: 0.4733\n",
      "Epoch 1, Batch 3170/5285, Loss: 0.5891\n",
      "Epoch 1, Batch 3180/5285, Loss: 0.3751\n",
      "Epoch 1, Batch 3190/5285, Loss: 0.5525\n",
      "Epoch 1, Batch 3200/5285, Loss: 0.1932\n",
      "Epoch 1, Batch 3210/5285, Loss: 0.4580\n",
      "Epoch 1, Batch 3220/5285, Loss: 0.2575\n",
      "Epoch 1, Batch 3230/5285, Loss: 0.2683\n",
      "Epoch 1, Batch 3240/5285, Loss: 0.5235\n",
      "Epoch 1, Batch 3250/5285, Loss: 0.4122\n",
      "Epoch 1, Batch 3260/5285, Loss: 0.4297\n",
      "Epoch 1, Batch 3270/5285, Loss: 0.7173\n",
      "Epoch 1, Batch 3280/5285, Loss: 0.2853\n",
      "Epoch 1, Batch 3290/5285, Loss: 0.2304\n",
      "Epoch 1, Batch 3300/5285, Loss: 0.6431\n",
      "Epoch 1, Batch 3310/5285, Loss: 0.5073\n",
      "Epoch 1, Batch 3320/5285, Loss: 0.1622\n",
      "Epoch 1, Batch 3330/5285, Loss: 0.4597\n",
      "Epoch 1, Batch 3340/5285, Loss: 0.4690\n",
      "Epoch 1, Batch 3350/5285, Loss: 0.3556\n",
      "Epoch 1, Batch 3360/5285, Loss: 0.5237\n",
      "Epoch 1, Batch 3370/5285, Loss: 0.2700\n",
      "Epoch 1, Batch 3380/5285, Loss: 0.3537\n",
      "Epoch 1, Batch 3390/5285, Loss: 0.4128\n",
      "Epoch 1, Batch 3400/5285, Loss: 0.7412\n",
      "Epoch 1, Batch 3410/5285, Loss: 0.2964\n",
      "Epoch 1, Batch 3420/5285, Loss: 0.3771\n",
      "Epoch 1, Batch 3430/5285, Loss: 0.4236\n",
      "Epoch 1, Batch 3440/5285, Loss: 0.5627\n",
      "Epoch 1, Batch 3450/5285, Loss: 0.2880\n",
      "Epoch 1, Batch 3460/5285, Loss: 0.2963\n",
      "Epoch 1, Batch 3470/5285, Loss: 0.5893\n",
      "Epoch 1, Batch 3480/5285, Loss: 0.6740\n",
      "Epoch 1, Batch 3490/5285, Loss: 0.3011\n",
      "Epoch 1, Batch 3500/5285, Loss: 0.3380\n",
      "Epoch 1, Batch 3510/5285, Loss: 0.4903\n",
      "Epoch 1, Batch 3520/5285, Loss: 0.1969\n",
      "Epoch 1, Batch 3530/5285, Loss: 0.5264\n",
      "Epoch 1, Batch 3540/5285, Loss: 0.4230\n",
      "Epoch 1, Batch 3550/5285, Loss: 0.4775\n",
      "Epoch 1, Batch 3560/5285, Loss: 0.5556\n",
      "Epoch 1, Batch 3570/5285, Loss: 0.8530\n",
      "Epoch 1, Batch 3580/5285, Loss: 0.4576\n",
      "Epoch 1, Batch 3590/5285, Loss: 0.5806\n",
      "Epoch 1, Batch 3600/5285, Loss: 0.6110\n",
      "Epoch 1, Batch 3610/5285, Loss: 0.3610\n",
      "Epoch 1, Batch 3620/5285, Loss: 0.1653\n",
      "Epoch 1, Batch 3630/5285, Loss: 0.3112\n",
      "Epoch 1, Batch 3640/5285, Loss: 0.3233\n",
      "Epoch 1, Batch 3650/5285, Loss: 0.2816\n",
      "Epoch 1, Batch 3660/5285, Loss: 0.5422\n",
      "Epoch 1, Batch 3670/5285, Loss: 0.1936\n",
      "Epoch 1, Batch 3680/5285, Loss: 0.4301\n",
      "Epoch 1, Batch 3690/5285, Loss: 0.4949\n",
      "Epoch 1, Batch 3700/5285, Loss: 0.2058\n",
      "Epoch 1, Batch 3710/5285, Loss: 0.1377\n",
      "Epoch 1, Batch 3720/5285, Loss: 0.6832\n",
      "Epoch 1, Batch 3730/5285, Loss: 0.5069\n",
      "Epoch 1, Batch 3740/5285, Loss: 0.5916\n",
      "Epoch 1, Batch 3750/5285, Loss: 0.3621\n",
      "Epoch 1, Batch 3760/5285, Loss: 0.4408\n",
      "Epoch 1, Batch 3770/5285, Loss: 1.1708\n",
      "Epoch 1, Batch 3780/5285, Loss: 0.3596\n",
      "Epoch 1, Batch 3790/5285, Loss: 0.6597\n",
      "Epoch 1, Batch 3800/5285, Loss: 0.2097\n",
      "Epoch 1, Batch 3810/5285, Loss: 0.3275\n",
      "Epoch 1, Batch 3820/5285, Loss: 0.5396\n",
      "Epoch 1, Batch 3830/5285, Loss: 0.5765\n",
      "Epoch 1, Batch 3840/5285, Loss: 0.6037\n",
      "Epoch 1, Batch 3850/5285, Loss: 0.5589\n",
      "Epoch 1, Batch 3860/5285, Loss: 0.3800\n",
      "Epoch 1, Batch 3870/5285, Loss: 0.3834\n",
      "Epoch 1, Batch 3880/5285, Loss: 0.4161\n",
      "Epoch 1, Batch 3890/5285, Loss: 0.5113\n",
      "Epoch 1, Batch 3900/5285, Loss: 0.4357\n",
      "Epoch 1, Batch 3910/5285, Loss: 0.5258\n",
      "Epoch 1, Batch 3920/5285, Loss: 0.3938\n",
      "Epoch 1, Batch 3930/5285, Loss: 0.3017\n",
      "Epoch 1, Batch 3940/5285, Loss: 0.4387\n",
      "Epoch 1, Batch 3950/5285, Loss: 0.7262\n",
      "Epoch 1, Batch 3960/5285, Loss: 0.4184\n",
      "Epoch 1, Batch 3970/5285, Loss: 0.6226\n",
      "Epoch 1, Batch 3980/5285, Loss: 0.6509\n",
      "Epoch 1, Batch 3990/5285, Loss: 0.2274\n",
      "Epoch 1, Batch 4000/5285, Loss: 0.2760\n",
      "Epoch 1, Batch 4010/5285, Loss: 0.5362\n",
      "Epoch 1, Batch 4020/5285, Loss: 0.6885\n",
      "Epoch 1, Batch 4030/5285, Loss: 0.3775\n",
      "Epoch 1, Batch 4040/5285, Loss: 0.8187\n",
      "Epoch 1, Batch 4050/5285, Loss: 0.8030\n",
      "Epoch 1, Batch 4060/5285, Loss: 0.9195\n",
      "Epoch 1, Batch 4070/5285, Loss: 0.4746\n",
      "Epoch 1, Batch 4080/5285, Loss: 0.5749\n",
      "Epoch 1, Batch 4090/5285, Loss: 0.6424\n",
      "Epoch 1, Batch 4100/5285, Loss: 0.5669\n",
      "Epoch 1, Batch 4110/5285, Loss: 0.4769\n",
      "Epoch 1, Batch 4120/5285, Loss: 0.7032\n",
      "Epoch 1, Batch 4130/5285, Loss: 0.4132\n",
      "Epoch 1, Batch 4140/5285, Loss: 0.5548\n",
      "Epoch 1, Batch 4150/5285, Loss: 0.5655\n",
      "Epoch 1, Batch 4160/5285, Loss: 0.4002\n",
      "Epoch 1, Batch 4170/5285, Loss: 0.7398\n",
      "Epoch 1, Batch 4180/5285, Loss: 0.5205\n",
      "Epoch 1, Batch 4190/5285, Loss: 0.4600\n",
      "Epoch 1, Batch 4200/5285, Loss: 0.1869\n",
      "Epoch 1, Batch 4210/5285, Loss: 0.1931\n",
      "Epoch 1, Batch 4220/5285, Loss: 0.2977\n",
      "Epoch 1, Batch 4230/5285, Loss: 0.6217\n",
      "Epoch 1, Batch 4240/5285, Loss: 0.5094\n",
      "Epoch 1, Batch 4250/5285, Loss: 0.4193\n",
      "Epoch 1, Batch 4260/5285, Loss: 0.3046\n",
      "Epoch 1, Batch 4270/5285, Loss: 0.7501\n",
      "Epoch 1, Batch 4280/5285, Loss: 0.3430\n",
      "Epoch 1, Batch 4290/5285, Loss: 0.2806\n",
      "Epoch 1, Batch 4300/5285, Loss: 0.1408\n",
      "Epoch 1, Batch 4310/5285, Loss: 0.4130\n",
      "Epoch 1, Batch 4320/5285, Loss: 0.1858\n",
      "Epoch 1, Batch 4330/5285, Loss: 0.3320\n",
      "Epoch 1, Batch 4340/5285, Loss: 0.7011\n",
      "Epoch 1, Batch 4350/5285, Loss: 0.4207\n",
      "Epoch 1, Batch 4360/5285, Loss: 0.6614\n",
      "Epoch 1, Batch 4370/5285, Loss: 0.2367\n",
      "Epoch 1, Batch 4380/5285, Loss: 0.3436\n",
      "Epoch 1, Batch 4390/5285, Loss: 0.3362\n",
      "Epoch 1, Batch 4400/5285, Loss: 0.4723\n",
      "Epoch 1, Batch 4410/5285, Loss: 0.8044\n",
      "Epoch 1, Batch 4420/5285, Loss: 0.4650\n",
      "Epoch 1, Batch 4430/5285, Loss: 0.4769\n",
      "Epoch 1, Batch 4440/5285, Loss: 0.6463\n",
      "Epoch 1, Batch 4450/5285, Loss: 0.6033\n",
      "Epoch 1, Batch 4460/5285, Loss: 0.4956\n",
      "Epoch 1, Batch 4470/5285, Loss: 0.2374\n",
      "Epoch 1, Batch 4480/5285, Loss: 0.4477\n",
      "Epoch 1, Batch 4490/5285, Loss: 0.4399\n",
      "Epoch 1, Batch 4500/5285, Loss: 0.6466\n",
      "Epoch 1, Batch 4510/5285, Loss: 0.4349\n",
      "Epoch 1, Batch 4520/5285, Loss: 0.4632\n",
      "Epoch 1, Batch 4530/5285, Loss: 0.4126\n",
      "Epoch 1, Batch 4540/5285, Loss: 0.8210\n",
      "Epoch 1, Batch 4550/5285, Loss: 0.4611\n",
      "Epoch 1, Batch 4560/5285, Loss: 0.3417\n",
      "Epoch 1, Batch 4570/5285, Loss: 0.2547\n",
      "Epoch 1, Batch 4580/5285, Loss: 0.3742\n",
      "Epoch 1, Batch 4590/5285, Loss: 0.5763\n",
      "Epoch 1, Batch 4600/5285, Loss: 0.1915\n",
      "Epoch 1, Batch 4610/5285, Loss: 0.6591\n",
      "Epoch 1, Batch 4620/5285, Loss: 0.3312\n",
      "Epoch 1, Batch 4630/5285, Loss: 0.3991\n",
      "Epoch 1, Batch 4640/5285, Loss: 0.2622\n",
      "Epoch 1, Batch 4650/5285, Loss: 0.4565\n",
      "Epoch 1, Batch 4660/5285, Loss: 0.4557\n",
      "Epoch 1, Batch 4670/5285, Loss: 0.2725\n",
      "Epoch 1, Batch 4680/5285, Loss: 0.2136\n",
      "Epoch 1, Batch 4690/5285, Loss: 0.3631\n",
      "Epoch 1, Batch 4700/5285, Loss: 0.2427\n",
      "Epoch 1, Batch 4710/5285, Loss: 0.4198\n",
      "Epoch 1, Batch 4720/5285, Loss: 0.7672\n",
      "Epoch 1, Batch 4730/5285, Loss: 0.2186\n",
      "Epoch 1, Batch 4740/5285, Loss: 0.2418\n",
      "Epoch 1, Batch 4750/5285, Loss: 0.3417\n",
      "Epoch 1, Batch 4760/5285, Loss: 0.1224\n",
      "Epoch 1, Batch 4770/5285, Loss: 0.4135\n",
      "Epoch 1, Batch 4780/5285, Loss: 0.2978\n",
      "Epoch 1, Batch 4790/5285, Loss: 0.2509\n",
      "Epoch 1, Batch 4800/5285, Loss: 0.3229\n",
      "Epoch 1, Batch 4810/5285, Loss: 0.5791\n",
      "Epoch 1, Batch 4820/5285, Loss: 0.4429\n",
      "Epoch 1, Batch 4830/5285, Loss: 0.1951\n",
      "Epoch 1, Batch 4840/5285, Loss: 0.3641\n",
      "Epoch 1, Batch 4850/5285, Loss: 0.5073\n",
      "Epoch 1, Batch 4860/5285, Loss: 0.3474\n",
      "Epoch 1, Batch 4870/5285, Loss: 0.4550\n",
      "Epoch 1, Batch 4880/5285, Loss: 0.8388\n",
      "Epoch 1, Batch 4890/5285, Loss: 0.3953\n",
      "Epoch 1, Batch 4900/5285, Loss: 0.1196\n",
      "Epoch 1, Batch 4910/5285, Loss: 0.2206\n",
      "Epoch 1, Batch 4920/5285, Loss: 0.4593\n",
      "Epoch 1, Batch 4930/5285, Loss: 0.3059\n",
      "Epoch 1, Batch 4940/5285, Loss: 0.2804\n",
      "Epoch 1, Batch 4950/5285, Loss: 0.5380\n",
      "Epoch 1, Batch 4960/5285, Loss: 0.3393\n",
      "Epoch 1, Batch 4970/5285, Loss: 1.0446\n",
      "Epoch 1, Batch 4980/5285, Loss: 0.4189\n",
      "Epoch 1, Batch 4990/5285, Loss: 0.4283\n",
      "Epoch 1, Batch 5000/5285, Loss: 0.4387\n",
      "Epoch 1, Batch 5010/5285, Loss: 0.3415\n",
      "Epoch 1, Batch 5020/5285, Loss: 0.3746\n",
      "Epoch 1, Batch 5030/5285, Loss: 0.2590\n",
      "Epoch 1, Batch 5040/5285, Loss: 0.2883\n",
      "Epoch 1, Batch 5050/5285, Loss: 0.4382\n",
      "Epoch 1, Batch 5060/5285, Loss: 0.4956\n",
      "Epoch 1, Batch 5070/5285, Loss: 0.3020\n",
      "Epoch 1, Batch 5080/5285, Loss: 0.3285\n",
      "Epoch 1, Batch 5090/5285, Loss: 0.7826\n",
      "Epoch 1, Batch 5100/5285, Loss: 0.4152\n",
      "Epoch 1, Batch 5110/5285, Loss: 0.4976\n",
      "Epoch 1, Batch 5120/5285, Loss: 0.3537\n",
      "Epoch 1, Batch 5130/5285, Loss: 0.2315\n",
      "Epoch 1, Batch 5140/5285, Loss: 0.2531\n",
      "Epoch 1, Batch 5150/5285, Loss: 0.4684\n",
      "Epoch 1, Batch 5160/5285, Loss: 0.6454\n",
      "Epoch 1, Batch 5170/5285, Loss: 0.3360\n",
      "Epoch 1, Batch 5180/5285, Loss: 0.5516\n",
      "Epoch 1, Batch 5190/5285, Loss: 0.2984\n",
      "Epoch 1, Batch 5200/5285, Loss: 0.6264\n",
      "Epoch 1, Batch 5210/5285, Loss: 0.2823\n",
      "Epoch 1, Batch 5220/5285, Loss: 0.2434\n",
      "Epoch 1, Batch 5230/5285, Loss: 0.4816\n",
      "Epoch 1, Batch 5240/5285, Loss: 0.5316\n",
      "Epoch 1, Batch 5250/5285, Loss: 0.2743\n",
      "Epoch 1, Batch 5260/5285, Loss: 0.5082\n",
      "Epoch 1, Batch 5270/5285, Loss: 0.4681\n",
      "Epoch 1, Batch 5280/5285, Loss: 0.5148\n",
      "Epoch 1, Average Loss: 0.4683, Accuracy: 79.48%\n",
      "Epoch 2, Batch 10/5285, Loss: 0.5743\n",
      "Epoch 2, Batch 20/5285, Loss: 0.4383\n",
      "Epoch 2, Batch 30/5285, Loss: 0.6351\n",
      "Epoch 2, Batch 40/5285, Loss: 0.6485\n",
      "Epoch 2, Batch 50/5285, Loss: 0.3945\n",
      "Epoch 2, Batch 60/5285, Loss: 0.4025\n",
      "Epoch 2, Batch 70/5285, Loss: 0.2352\n",
      "Epoch 2, Batch 80/5285, Loss: 0.2037\n",
      "Epoch 2, Batch 90/5285, Loss: 0.2687\n",
      "Epoch 2, Batch 100/5285, Loss: 0.2013\n",
      "Epoch 2, Batch 110/5285, Loss: 0.4144\n",
      "Epoch 2, Batch 120/5285, Loss: 0.6098\n",
      "Epoch 2, Batch 130/5285, Loss: 0.3799\n",
      "Epoch 2, Batch 140/5285, Loss: 0.4052\n",
      "Epoch 2, Batch 150/5285, Loss: 0.5172\n",
      "Epoch 2, Batch 160/5285, Loss: 0.2584\n",
      "Epoch 2, Batch 170/5285, Loss: 0.2973\n",
      "Epoch 2, Batch 180/5285, Loss: 0.7717\n",
      "Epoch 2, Batch 190/5285, Loss: 0.6529\n",
      "Epoch 2, Batch 200/5285, Loss: 0.5112\n",
      "Epoch 2, Batch 210/5285, Loss: 0.2529\n",
      "Epoch 2, Batch 220/5285, Loss: 0.3460\n",
      "Epoch 2, Batch 230/5285, Loss: 0.8279\n",
      "Epoch 2, Batch 240/5285, Loss: 0.2724\n",
      "Epoch 2, Batch 250/5285, Loss: 0.5153\n",
      "Epoch 2, Batch 260/5285, Loss: 0.5013\n",
      "Epoch 2, Batch 270/5285, Loss: 0.4877\n",
      "Epoch 2, Batch 280/5285, Loss: 0.3634\n",
      "Epoch 2, Batch 290/5285, Loss: 0.4376\n",
      "Epoch 2, Batch 300/5285, Loss: 0.5232\n",
      "Epoch 2, Batch 310/5285, Loss: 0.3230\n",
      "Epoch 2, Batch 320/5285, Loss: 0.3994\n",
      "Epoch 2, Batch 330/5285, Loss: 0.2664\n",
      "Epoch 2, Batch 340/5285, Loss: 0.3629\n",
      "Epoch 2, Batch 350/5285, Loss: 0.4359\n",
      "Epoch 2, Batch 360/5285, Loss: 0.2246\n",
      "Epoch 2, Batch 370/5285, Loss: 0.5586\n",
      "Epoch 2, Batch 380/5285, Loss: 0.4409\n",
      "Epoch 2, Batch 390/5285, Loss: 0.4717\n",
      "Epoch 2, Batch 400/5285, Loss: 0.5919\n",
      "Epoch 2, Batch 410/5285, Loss: 0.6600\n",
      "Epoch 2, Batch 420/5285, Loss: 0.5330\n",
      "Epoch 2, Batch 430/5285, Loss: 0.5003\n",
      "Epoch 2, Batch 440/5285, Loss: 0.5880\n",
      "Epoch 2, Batch 450/5285, Loss: 0.4477\n",
      "Epoch 2, Batch 460/5285, Loss: 0.4277\n",
      "Epoch 2, Batch 470/5285, Loss: 0.1988\n",
      "Epoch 2, Batch 480/5285, Loss: 0.3850\n",
      "Epoch 2, Batch 490/5285, Loss: 0.4624\n",
      "Epoch 2, Batch 500/5285, Loss: 0.4864\n",
      "Epoch 2, Batch 510/5285, Loss: 0.4555\n",
      "Epoch 2, Batch 520/5285, Loss: 0.2464\n",
      "Epoch 2, Batch 530/5285, Loss: 0.2537\n",
      "Epoch 2, Batch 540/5285, Loss: 0.2473\n",
      "Epoch 2, Batch 550/5285, Loss: 0.2409\n",
      "Epoch 2, Batch 560/5285, Loss: 0.4729\n",
      "Epoch 2, Batch 570/5285, Loss: 0.8742\n",
      "Epoch 2, Batch 580/5285, Loss: 0.4158\n",
      "Epoch 2, Batch 590/5285, Loss: 0.7106\n",
      "Epoch 2, Batch 600/5285, Loss: 0.1809\n",
      "Epoch 2, Batch 610/5285, Loss: 0.2243\n",
      "Epoch 2, Batch 620/5285, Loss: 0.3012\n",
      "Epoch 2, Batch 630/5285, Loss: 0.2224\n",
      "Epoch 2, Batch 640/5285, Loss: 0.5540\n",
      "Epoch 2, Batch 650/5285, Loss: 0.5481\n",
      "Epoch 2, Batch 660/5285, Loss: 0.1924\n",
      "Epoch 2, Batch 670/5285, Loss: 0.2474\n",
      "Epoch 2, Batch 680/5285, Loss: 0.1363\n",
      "Epoch 2, Batch 690/5285, Loss: 0.3479\n",
      "Epoch 2, Batch 700/5285, Loss: 0.7354\n",
      "Epoch 2, Batch 710/5285, Loss: 0.7654\n",
      "Epoch 2, Batch 720/5285, Loss: 0.2631\n",
      "Epoch 2, Batch 730/5285, Loss: 0.6249\n",
      "Epoch 2, Batch 740/5285, Loss: 0.2225\n",
      "Epoch 2, Batch 750/5285, Loss: 0.3203\n",
      "Epoch 2, Batch 760/5285, Loss: 0.2082\n",
      "Epoch 2, Batch 770/5285, Loss: 0.7354\n",
      "Epoch 2, Batch 780/5285, Loss: 0.4490\n",
      "Epoch 2, Batch 790/5285, Loss: 0.6750\n",
      "Epoch 2, Batch 800/5285, Loss: 0.2587\n",
      "Epoch 2, Batch 810/5285, Loss: 0.2479\n",
      "Epoch 2, Batch 820/5285, Loss: 0.2907\n",
      "Epoch 2, Batch 830/5285, Loss: 0.6271\n",
      "Epoch 2, Batch 840/5285, Loss: 0.4029\n",
      "Epoch 2, Batch 850/5285, Loss: 0.3849\n",
      "Epoch 2, Batch 860/5285, Loss: 0.8058\n",
      "Epoch 2, Batch 870/5285, Loss: 0.3016\n",
      "Epoch 2, Batch 880/5285, Loss: 0.4876\n",
      "Epoch 2, Batch 890/5285, Loss: 0.3759\n",
      "Epoch 2, Batch 900/5285, Loss: 0.2866\n",
      "Epoch 2, Batch 910/5285, Loss: 0.5471\n",
      "Epoch 2, Batch 920/5285, Loss: 0.2972\n",
      "Epoch 2, Batch 930/5285, Loss: 0.2405\n",
      "Epoch 2, Batch 940/5285, Loss: 0.2543\n",
      "Epoch 2, Batch 950/5285, Loss: 0.4360\n",
      "Epoch 2, Batch 960/5285, Loss: 0.1730\n",
      "Epoch 2, Batch 970/5285, Loss: 0.2382\n",
      "Epoch 2, Batch 980/5285, Loss: 0.3986\n",
      "Epoch 2, Batch 990/5285, Loss: 0.4325\n",
      "Epoch 2, Batch 1000/5285, Loss: 0.2205\n",
      "Epoch 2, Batch 1010/5285, Loss: 0.6833\n",
      "Epoch 2, Batch 1020/5285, Loss: 0.1063\n",
      "Epoch 2, Batch 1030/5285, Loss: 0.3677\n",
      "Epoch 2, Batch 1040/5285, Loss: 0.3187\n",
      "Epoch 2, Batch 1050/5285, Loss: 0.5345\n",
      "Epoch 2, Batch 1060/5285, Loss: 0.2246\n",
      "Epoch 2, Batch 1070/5285, Loss: 0.1010\n",
      "Epoch 2, Batch 1080/5285, Loss: 0.3828\n",
      "Epoch 2, Batch 1090/5285, Loss: 0.5077\n",
      "Epoch 2, Batch 1100/5285, Loss: 0.5392\n",
      "Epoch 2, Batch 1110/5285, Loss: 0.4884\n",
      "Epoch 2, Batch 1120/5285, Loss: 0.4978\n",
      "Epoch 2, Batch 1130/5285, Loss: 0.4953\n",
      "Epoch 2, Batch 1140/5285, Loss: 0.3158\n",
      "Epoch 2, Batch 1150/5285, Loss: 0.3620\n",
      "Epoch 2, Batch 1160/5285, Loss: 0.3404\n",
      "Epoch 2, Batch 1170/5285, Loss: 0.4163\n",
      "Epoch 2, Batch 1180/5285, Loss: 0.3749\n",
      "Epoch 2, Batch 1190/5285, Loss: 0.6343\n",
      "Epoch 2, Batch 1200/5285, Loss: 0.6635\n",
      "Epoch 2, Batch 1210/5285, Loss: 0.4256\n",
      "Epoch 2, Batch 1220/5285, Loss: 0.4703\n",
      "Epoch 2, Batch 1230/5285, Loss: 0.5865\n",
      "Epoch 2, Batch 1240/5285, Loss: 0.2978\n",
      "Epoch 2, Batch 1250/5285, Loss: 0.2448\n",
      "Epoch 2, Batch 1260/5285, Loss: 0.1278\n",
      "Epoch 2, Batch 1270/5285, Loss: 0.3294\n",
      "Epoch 2, Batch 1280/5285, Loss: 0.8664\n",
      "Epoch 2, Batch 1290/5285, Loss: 0.2949\n",
      "Epoch 2, Batch 1300/5285, Loss: 0.5155\n",
      "Epoch 2, Batch 1310/5285, Loss: 0.4214\n",
      "Epoch 2, Batch 1320/5285, Loss: 0.1116\n",
      "Epoch 2, Batch 1330/5285, Loss: 0.4565\n",
      "Epoch 2, Batch 1340/5285, Loss: 0.1083\n",
      "Epoch 2, Batch 1350/5285, Loss: 0.6036\n",
      "Epoch 2, Batch 1360/5285, Loss: 0.2479\n",
      "Epoch 2, Batch 1370/5285, Loss: 0.5002\n",
      "Epoch 2, Batch 1380/5285, Loss: 0.2774\n",
      "Epoch 2, Batch 1390/5285, Loss: 0.2551\n",
      "Epoch 2, Batch 1400/5285, Loss: 0.3968\n",
      "Epoch 2, Batch 1410/5285, Loss: 0.4865\n",
      "Epoch 2, Batch 1420/5285, Loss: 0.4414\n",
      "Epoch 2, Batch 1430/5285, Loss: 0.4913\n",
      "Epoch 2, Batch 1440/5285, Loss: 0.5959\n",
      "Epoch 2, Batch 1450/5285, Loss: 0.4220\n",
      "Epoch 2, Batch 1460/5285, Loss: 0.1430\n",
      "Epoch 2, Batch 1470/5285, Loss: 0.3013\n",
      "Epoch 2, Batch 1480/5285, Loss: 0.2582\n",
      "Epoch 2, Batch 1490/5285, Loss: 0.6264\n",
      "Epoch 2, Batch 1500/5285, Loss: 0.2344\n",
      "Epoch 2, Batch 1510/5285, Loss: 0.5060\n",
      "Epoch 2, Batch 1520/5285, Loss: 0.3469\n",
      "Epoch 2, Batch 1530/5285, Loss: 0.2599\n",
      "Epoch 2, Batch 1540/5285, Loss: 0.3526\n",
      "Epoch 2, Batch 1550/5285, Loss: 0.4923\n",
      "Epoch 2, Batch 1560/5285, Loss: 0.6220\n",
      "Epoch 2, Batch 1570/5285, Loss: 0.6761\n",
      "Epoch 2, Batch 1580/5285, Loss: 0.7410\n",
      "Epoch 2, Batch 1590/5285, Loss: 0.2282\n",
      "Epoch 2, Batch 1600/5285, Loss: 0.4063\n",
      "Epoch 2, Batch 1610/5285, Loss: 0.2570\n",
      "Epoch 2, Batch 1620/5285, Loss: 0.5148\n",
      "Epoch 2, Batch 1630/5285, Loss: 0.2545\n",
      "Epoch 2, Batch 1640/5285, Loss: 0.3186\n",
      "Epoch 2, Batch 1650/5285, Loss: 0.1442\n",
      "Epoch 2, Batch 1660/5285, Loss: 0.1742\n",
      "Epoch 2, Batch 1670/5285, Loss: 0.1476\n",
      "Epoch 2, Batch 1680/5285, Loss: 0.4325\n",
      "Epoch 2, Batch 1690/5285, Loss: 0.5845\n",
      "Epoch 2, Batch 1700/5285, Loss: 0.3551\n",
      "Epoch 2, Batch 1710/5285, Loss: 0.5161\n",
      "Epoch 2, Batch 1720/5285, Loss: 0.3635\n",
      "Epoch 2, Batch 1730/5285, Loss: 0.3795\n",
      "Epoch 2, Batch 1740/5285, Loss: 0.3699\n",
      "Epoch 2, Batch 1750/5285, Loss: 0.0864\n",
      "Epoch 2, Batch 1760/5285, Loss: 0.5824\n",
      "Epoch 2, Batch 1770/5285, Loss: 0.3744\n",
      "Epoch 2, Batch 1780/5285, Loss: 0.3539\n",
      "Epoch 2, Batch 1790/5285, Loss: 0.4032\n",
      "Epoch 2, Batch 1800/5285, Loss: 0.4254\n",
      "Epoch 2, Batch 1810/5285, Loss: 0.6774\n",
      "Epoch 2, Batch 1820/5285, Loss: 0.3869\n",
      "Epoch 2, Batch 1830/5285, Loss: 0.3332\n",
      "Epoch 2, Batch 1840/5285, Loss: 0.4431\n",
      "Epoch 2, Batch 1850/5285, Loss: 0.1705\n",
      "Epoch 2, Batch 1860/5285, Loss: 0.6216\n",
      "Epoch 2, Batch 1870/5285, Loss: 0.3613\n",
      "Epoch 2, Batch 1880/5285, Loss: 0.4139\n",
      "Epoch 2, Batch 1890/5285, Loss: 0.6566\n",
      "Epoch 2, Batch 1900/5285, Loss: 0.2869\n",
      "Epoch 2, Batch 1910/5285, Loss: 0.6049\n",
      "Epoch 2, Batch 1920/5285, Loss: 0.1773\n",
      "Epoch 2, Batch 1930/5285, Loss: 0.5078\n",
      "Epoch 2, Batch 1940/5285, Loss: 0.6088\n",
      "Epoch 2, Batch 1950/5285, Loss: 0.3049\n",
      "Epoch 2, Batch 1960/5285, Loss: 0.3136\n",
      "Epoch 2, Batch 1970/5285, Loss: 0.4974\n",
      "Epoch 2, Batch 1980/5285, Loss: 0.3297\n",
      "Epoch 2, Batch 1990/5285, Loss: 0.6637\n",
      "Epoch 2, Batch 2000/5285, Loss: 0.3257\n",
      "Epoch 2, Batch 2010/5285, Loss: 0.2402\n",
      "Epoch 2, Batch 2020/5285, Loss: 0.6842\n",
      "Epoch 2, Batch 2030/5285, Loss: 0.1436\n",
      "Epoch 2, Batch 2040/5285, Loss: 0.3203\n",
      "Epoch 2, Batch 2050/5285, Loss: 0.4279\n",
      "Epoch 2, Batch 2060/5285, Loss: 0.4536\n",
      "Epoch 2, Batch 2070/5285, Loss: 0.9789\n",
      "Epoch 2, Batch 2080/5285, Loss: 0.3274\n",
      "Epoch 2, Batch 2090/5285, Loss: 0.5345\n",
      "Epoch 2, Batch 2100/5285, Loss: 0.6430\n",
      "Epoch 2, Batch 2110/5285, Loss: 0.3988\n",
      "Epoch 2, Batch 2120/5285, Loss: 0.3694\n",
      "Epoch 2, Batch 2130/5285, Loss: 0.2983\n",
      "Epoch 2, Batch 2140/5285, Loss: 0.6801\n",
      "Epoch 2, Batch 2150/5285, Loss: 0.3141\n",
      "Epoch 2, Batch 2160/5285, Loss: 0.1429\n",
      "Epoch 2, Batch 2170/5285, Loss: 0.3328\n",
      "Epoch 2, Batch 2180/5285, Loss: 0.2442\n",
      "Epoch 2, Batch 2190/5285, Loss: 0.1707\n",
      "Epoch 2, Batch 2200/5285, Loss: 0.5263\n",
      "Epoch 2, Batch 2210/5285, Loss: 0.2020\n",
      "Epoch 2, Batch 2220/5285, Loss: 0.2428\n",
      "Epoch 2, Batch 2230/5285, Loss: 0.5682\n",
      "Epoch 2, Batch 2240/5285, Loss: 0.6085\n",
      "Epoch 2, Batch 2250/5285, Loss: 0.5776\n",
      "Epoch 2, Batch 2260/5285, Loss: 0.1615\n",
      "Epoch 2, Batch 2270/5285, Loss: 0.3518\n",
      "Epoch 2, Batch 2280/5285, Loss: 0.6977\n",
      "Epoch 2, Batch 2290/5285, Loss: 0.4312\n",
      "Epoch 2, Batch 2300/5285, Loss: 0.3909\n",
      "Epoch 2, Batch 2310/5285, Loss: 0.3108\n",
      "Epoch 2, Batch 2320/5285, Loss: 0.2597\n",
      "Epoch 2, Batch 2330/5285, Loss: 0.2921\n",
      "Epoch 2, Batch 2340/5285, Loss: 0.3388\n",
      "Epoch 2, Batch 2350/5285, Loss: 0.2820\n",
      "Epoch 2, Batch 2360/5285, Loss: 0.3350\n",
      "Epoch 2, Batch 2370/5285, Loss: 0.4694\n",
      "Epoch 2, Batch 2380/5285, Loss: 0.4546\n",
      "Epoch 2, Batch 2390/5285, Loss: 0.4545\n",
      "Epoch 2, Batch 2400/5285, Loss: 0.4574\n",
      "Epoch 2, Batch 2410/5285, Loss: 1.0670\n",
      "Epoch 2, Batch 2420/5285, Loss: 0.2953\n",
      "Epoch 2, Batch 2430/5285, Loss: 0.6632\n",
      "Epoch 2, Batch 2440/5285, Loss: 0.4935\n",
      "Epoch 2, Batch 2450/5285, Loss: 0.3445\n",
      "Epoch 2, Batch 2460/5285, Loss: 0.3729\n",
      "Epoch 2, Batch 2470/5285, Loss: 0.4630\n",
      "Epoch 2, Batch 2480/5285, Loss: 0.9155\n",
      "Epoch 2, Batch 2490/5285, Loss: 0.3541\n",
      "Epoch 2, Batch 2500/5285, Loss: 0.4399\n",
      "Epoch 2, Batch 2510/5285, Loss: 0.1863\n",
      "Epoch 2, Batch 2520/5285, Loss: 0.2423\n",
      "Epoch 2, Batch 2530/5285, Loss: 0.3096\n",
      "Epoch 2, Batch 2540/5285, Loss: 0.9302\n",
      "Epoch 2, Batch 2550/5285, Loss: 0.2985\n",
      "Epoch 2, Batch 2560/5285, Loss: 0.3103\n",
      "Epoch 2, Batch 2570/5285, Loss: 0.5303\n",
      "Epoch 2, Batch 2580/5285, Loss: 0.1394\n",
      "Epoch 2, Batch 2590/5285, Loss: 0.4736\n",
      "Epoch 2, Batch 2600/5285, Loss: 0.6455\n",
      "Epoch 2, Batch 2610/5285, Loss: 0.4719\n",
      "Epoch 2, Batch 2620/5285, Loss: 0.4104\n",
      "Epoch 2, Batch 2630/5285, Loss: 0.9775\n",
      "Epoch 2, Batch 2640/5285, Loss: 0.3839\n",
      "Epoch 2, Batch 2650/5285, Loss: 0.2366\n",
      "Epoch 2, Batch 2660/5285, Loss: 0.3304\n",
      "Epoch 2, Batch 2670/5285, Loss: 0.3813\n",
      "Epoch 2, Batch 2680/5285, Loss: 0.2245\n",
      "Epoch 2, Batch 2690/5285, Loss: 0.3431\n",
      "Epoch 2, Batch 2700/5285, Loss: 1.0041\n",
      "Epoch 2, Batch 2710/5285, Loss: 0.3872\n",
      "Epoch 2, Batch 2720/5285, Loss: 0.3933\n",
      "Epoch 2, Batch 2730/5285, Loss: 0.2042\n",
      "Epoch 2, Batch 2740/5285, Loss: 0.6197\n",
      "Epoch 2, Batch 2750/5285, Loss: 0.2751\n",
      "Epoch 2, Batch 2760/5285, Loss: 0.8285\n",
      "Epoch 2, Batch 2770/5285, Loss: 0.7351\n",
      "Epoch 2, Batch 2780/5285, Loss: 0.2481\n",
      "Epoch 2, Batch 2790/5285, Loss: 0.1443\n",
      "Epoch 2, Batch 2800/5285, Loss: 0.4189\n",
      "Epoch 2, Batch 2810/5285, Loss: 0.4311\n",
      "Epoch 2, Batch 2820/5285, Loss: 0.4638\n",
      "Epoch 2, Batch 2830/5285, Loss: 0.6046\n",
      "Epoch 2, Batch 2840/5285, Loss: 0.5231\n",
      "Epoch 2, Batch 2850/5285, Loss: 0.4165\n",
      "Epoch 2, Batch 2860/5285, Loss: 0.3723\n",
      "Epoch 2, Batch 2870/5285, Loss: 0.3225\n",
      "Epoch 2, Batch 2880/5285, Loss: 0.1898\n",
      "Epoch 2, Batch 2890/5285, Loss: 0.5643\n",
      "Epoch 2, Batch 2900/5285, Loss: 0.5273\n",
      "Epoch 2, Batch 2910/5285, Loss: 0.2736\n",
      "Epoch 2, Batch 2920/5285, Loss: 0.1516\n",
      "Epoch 2, Batch 2930/5285, Loss: 0.3035\n",
      "Epoch 2, Batch 2940/5285, Loss: 0.5318\n",
      "Epoch 2, Batch 2950/5285, Loss: 0.2877\n",
      "Epoch 2, Batch 2960/5285, Loss: 0.3917\n",
      "Epoch 2, Batch 2970/5285, Loss: 0.0864\n",
      "Epoch 2, Batch 2980/5285, Loss: 0.2855\n",
      "Epoch 2, Batch 2990/5285, Loss: 0.3859\n",
      "Epoch 2, Batch 3000/5285, Loss: 0.4866\n",
      "Epoch 2, Batch 3010/5285, Loss: 0.3824\n",
      "Epoch 2, Batch 3020/5285, Loss: 0.4785\n",
      "Epoch 2, Batch 3030/5285, Loss: 0.5044\n",
      "Epoch 2, Batch 3040/5285, Loss: 0.2866\n",
      "Epoch 2, Batch 3050/5285, Loss: 0.2291\n",
      "Epoch 2, Batch 3060/5285, Loss: 0.6458\n",
      "Epoch 2, Batch 3070/5285, Loss: 0.3918\n",
      "Epoch 2, Batch 3080/5285, Loss: 0.2246\n",
      "Epoch 2, Batch 3090/5285, Loss: 0.5593\n",
      "Epoch 2, Batch 3100/5285, Loss: 0.4818\n",
      "Epoch 2, Batch 3110/5285, Loss: 0.4385\n",
      "Epoch 2, Batch 3120/5285, Loss: 0.3961\n",
      "Epoch 2, Batch 3130/5285, Loss: 0.2231\n",
      "Epoch 2, Batch 3140/5285, Loss: 0.4129\n",
      "Epoch 2, Batch 3150/5285, Loss: 0.4428\n",
      "Epoch 2, Batch 3160/5285, Loss: 0.4235\n",
      "Epoch 2, Batch 3170/5285, Loss: 0.2054\n",
      "Epoch 2, Batch 3180/5285, Loss: 0.7317\n",
      "Epoch 2, Batch 3190/5285, Loss: 0.3288\n",
      "Epoch 2, Batch 3200/5285, Loss: 0.4583\n",
      "Epoch 2, Batch 3210/5285, Loss: 0.6496\n",
      "Epoch 2, Batch 3220/5285, Loss: 0.4062\n",
      "Epoch 2, Batch 3230/5285, Loss: 0.5108\n",
      "Epoch 2, Batch 3240/5285, Loss: 0.4324\n",
      "Epoch 2, Batch 3250/5285, Loss: 0.5306\n",
      "Epoch 2, Batch 3260/5285, Loss: 0.5554\n",
      "Epoch 2, Batch 3270/5285, Loss: 0.2690\n",
      "Epoch 2, Batch 3280/5285, Loss: 0.5235\n",
      "Epoch 2, Batch 3290/5285, Loss: 0.3236\n",
      "Epoch 2, Batch 3300/5285, Loss: 0.3274\n",
      "Epoch 2, Batch 3310/5285, Loss: 0.4257\n",
      "Epoch 2, Batch 3320/5285, Loss: 0.3380\n",
      "Epoch 2, Batch 3330/5285, Loss: 0.1722\n",
      "Epoch 2, Batch 3340/5285, Loss: 0.4986\n",
      "Epoch 2, Batch 3350/5285, Loss: 0.2807\n",
      "Epoch 2, Batch 3360/5285, Loss: 0.6151\n",
      "Epoch 2, Batch 3370/5285, Loss: 0.2641\n",
      "Epoch 2, Batch 3380/5285, Loss: 0.4449\n",
      "Epoch 2, Batch 3390/5285, Loss: 0.1395\n",
      "Epoch 2, Batch 3400/5285, Loss: 0.5913\n",
      "Epoch 2, Batch 3410/5285, Loss: 0.6076\n",
      "Epoch 2, Batch 3420/5285, Loss: 0.2661\n",
      "Epoch 2, Batch 3430/5285, Loss: 0.9976\n",
      "Epoch 2, Batch 3440/5285, Loss: 0.5097\n",
      "Epoch 2, Batch 3450/5285, Loss: 0.4458\n",
      "Epoch 2, Batch 3460/5285, Loss: 0.6245\n",
      "Epoch 2, Batch 3470/5285, Loss: 0.3288\n",
      "Epoch 2, Batch 3480/5285, Loss: 0.3281\n",
      "Epoch 2, Batch 3490/5285, Loss: 0.3473\n",
      "Epoch 2, Batch 3500/5285, Loss: 0.3149\n",
      "Epoch 2, Batch 3510/5285, Loss: 0.3688\n",
      "Epoch 2, Batch 3520/5285, Loss: 0.4796\n",
      "Epoch 2, Batch 3530/5285, Loss: 0.3112\n",
      "Epoch 2, Batch 3540/5285, Loss: 0.2829\n",
      "Epoch 2, Batch 3550/5285, Loss: 0.4178\n",
      "Epoch 2, Batch 3560/5285, Loss: 0.2968\n",
      "Epoch 2, Batch 3570/5285, Loss: 0.3889\n",
      "Epoch 2, Batch 3580/5285, Loss: 0.3659\n",
      "Epoch 2, Batch 3590/5285, Loss: 0.0813\n",
      "Epoch 2, Batch 3600/5285, Loss: 0.4476\n",
      "Epoch 2, Batch 3610/5285, Loss: 0.4747\n",
      "Epoch 2, Batch 3620/5285, Loss: 0.1872\n",
      "Epoch 2, Batch 3630/5285, Loss: 0.2447\n",
      "Epoch 2, Batch 3640/5285, Loss: 0.4249\n",
      "Epoch 2, Batch 3650/5285, Loss: 0.3137\n",
      "Epoch 2, Batch 3660/5285, Loss: 0.3384\n",
      "Epoch 2, Batch 3670/5285, Loss: 0.5144\n",
      "Epoch 2, Batch 3680/5285, Loss: 0.3359\n",
      "Epoch 2, Batch 3690/5285, Loss: 0.2856\n",
      "Epoch 2, Batch 3700/5285, Loss: 0.5943\n",
      "Epoch 2, Batch 3710/5285, Loss: 0.1867\n",
      "Epoch 2, Batch 3720/5285, Loss: 0.5616\n",
      "Epoch 2, Batch 3730/5285, Loss: 0.1257\n",
      "Epoch 2, Batch 3740/5285, Loss: 0.2959\n",
      "Epoch 2, Batch 3750/5285, Loss: 0.3653\n",
      "Epoch 2, Batch 3760/5285, Loss: 0.5376\n",
      "Epoch 2, Batch 3770/5285, Loss: 0.3606\n",
      "Epoch 2, Batch 3780/5285, Loss: 0.3302\n",
      "Epoch 2, Batch 3790/5285, Loss: 0.5188\n",
      "Epoch 2, Batch 3800/5285, Loss: 0.2045\n",
      "Epoch 2, Batch 3810/5285, Loss: 0.3364\n",
      "Epoch 2, Batch 3820/5285, Loss: 0.3064\n",
      "Epoch 2, Batch 3830/5285, Loss: 0.2945\n",
      "Epoch 2, Batch 3840/5285, Loss: 0.3157\n",
      "Epoch 2, Batch 3850/5285, Loss: 0.5985\n",
      "Epoch 2, Batch 3860/5285, Loss: 0.4923\n",
      "Epoch 2, Batch 3870/5285, Loss: 0.1712\n",
      "Epoch 2, Batch 3880/5285, Loss: 0.1204\n",
      "Epoch 2, Batch 3890/5285, Loss: 0.1639\n",
      "Epoch 2, Batch 3900/5285, Loss: 0.4649\n",
      "Epoch 2, Batch 3910/5285, Loss: 0.5185\n",
      "Epoch 2, Batch 3920/5285, Loss: 0.5708\n",
      "Epoch 2, Batch 3930/5285, Loss: 0.4368\n",
      "Epoch 2, Batch 3940/5285, Loss: 0.4248\n",
      "Epoch 2, Batch 3950/5285, Loss: 0.3783\n",
      "Epoch 2, Batch 3960/5285, Loss: 0.4577\n",
      "Epoch 2, Batch 3970/5285, Loss: 0.3528\n",
      "Epoch 2, Batch 3980/5285, Loss: 0.2123\n",
      "Epoch 2, Batch 3990/5285, Loss: 0.4939\n",
      "Epoch 2, Batch 4000/5285, Loss: 0.8157\n",
      "Epoch 2, Batch 4010/5285, Loss: 0.7155\n",
      "Epoch 2, Batch 4020/5285, Loss: 0.3183\n",
      "Epoch 2, Batch 4030/5285, Loss: 0.3819\n",
      "Epoch 2, Batch 4040/5285, Loss: 0.2568\n",
      "Epoch 2, Batch 4050/5285, Loss: 0.1977\n",
      "Epoch 2, Batch 4060/5285, Loss: 0.1947\n",
      "Epoch 2, Batch 4070/5285, Loss: 0.4467\n",
      "Epoch 2, Batch 4080/5285, Loss: 0.2669\n",
      "Epoch 2, Batch 4090/5285, Loss: 0.3379\n",
      "Epoch 2, Batch 4100/5285, Loss: 0.2324\n",
      "Epoch 2, Batch 4110/5285, Loss: 0.3655\n",
      "Epoch 2, Batch 4120/5285, Loss: 0.1348\n",
      "Epoch 2, Batch 4130/5285, Loss: 0.3295\n",
      "Epoch 2, Batch 4140/5285, Loss: 0.4628\n",
      "Epoch 2, Batch 4150/5285, Loss: 0.3834\n",
      "Epoch 2, Batch 4160/5285, Loss: 0.4954\n",
      "Epoch 2, Batch 4170/5285, Loss: 0.2402\n",
      "Epoch 2, Batch 4180/5285, Loss: 0.7228\n",
      "Epoch 2, Batch 4190/5285, Loss: 0.6146\n",
      "Epoch 2, Batch 4200/5285, Loss: 0.3368\n",
      "Epoch 2, Batch 4210/5285, Loss: 0.3815\n",
      "Epoch 2, Batch 4220/5285, Loss: 0.3733\n",
      "Epoch 2, Batch 4230/5285, Loss: 0.2639\n",
      "Epoch 2, Batch 4240/5285, Loss: 0.1420\n",
      "Epoch 2, Batch 4250/5285, Loss: 0.1944\n",
      "Epoch 2, Batch 4260/5285, Loss: 0.3443\n",
      "Epoch 2, Batch 4270/5285, Loss: 0.1632\n",
      "Epoch 2, Batch 4280/5285, Loss: 0.5764\n",
      "Epoch 2, Batch 4290/5285, Loss: 0.3122\n",
      "Epoch 2, Batch 4300/5285, Loss: 0.6272\n",
      "Epoch 2, Batch 4310/5285, Loss: 0.1413\n",
      "Epoch 2, Batch 4320/5285, Loss: 0.5131\n",
      "Epoch 2, Batch 4330/5285, Loss: 0.3035\n",
      "Epoch 2, Batch 4340/5285, Loss: 0.4020\n",
      "Epoch 2, Batch 4350/5285, Loss: 0.3400\n",
      "Epoch 2, Batch 4360/5285, Loss: 0.5503\n",
      "Epoch 2, Batch 4370/5285, Loss: 0.2553\n",
      "Epoch 2, Batch 4380/5285, Loss: 0.2207\n",
      "Epoch 2, Batch 4390/5285, Loss: 0.5057\n",
      "Epoch 2, Batch 4400/5285, Loss: 0.3243\n",
      "Epoch 2, Batch 4410/5285, Loss: 0.4436\n",
      "Epoch 2, Batch 4420/5285, Loss: 0.6839\n",
      "Epoch 2, Batch 4430/5285, Loss: 0.5120\n",
      "Epoch 2, Batch 4440/5285, Loss: 0.5304\n",
      "Epoch 2, Batch 4450/5285, Loss: 0.4937\n",
      "Epoch 2, Batch 4460/5285, Loss: 0.9590\n",
      "Epoch 2, Batch 4470/5285, Loss: 0.1188\n",
      "Epoch 2, Batch 4480/5285, Loss: 0.8266\n",
      "Epoch 2, Batch 4490/5285, Loss: 0.6830\n",
      "Epoch 2, Batch 4500/5285, Loss: 0.4704\n",
      "Epoch 2, Batch 4510/5285, Loss: 0.3956\n",
      "Epoch 2, Batch 4520/5285, Loss: 0.3886\n",
      "Epoch 2, Batch 4530/5285, Loss: 0.5922\n",
      "Epoch 2, Batch 4540/5285, Loss: 0.3636\n",
      "Epoch 2, Batch 4550/5285, Loss: 1.1513\n",
      "Epoch 2, Batch 4560/5285, Loss: 0.6065\n",
      "Epoch 2, Batch 4570/5285, Loss: 0.5344\n",
      "Epoch 2, Batch 4580/5285, Loss: 0.7109\n",
      "Epoch 2, Batch 4590/5285, Loss: 0.2972\n",
      "Epoch 2, Batch 4600/5285, Loss: 0.4095\n",
      "Epoch 2, Batch 4610/5285, Loss: 0.5665\n",
      "Epoch 2, Batch 4620/5285, Loss: 0.2067\n",
      "Epoch 2, Batch 4630/5285, Loss: 0.5318\n",
      "Epoch 2, Batch 4640/5285, Loss: 0.5598\n",
      "Epoch 2, Batch 4650/5285, Loss: 0.5219\n",
      "Epoch 2, Batch 4660/5285, Loss: 0.5332\n",
      "Epoch 2, Batch 4670/5285, Loss: 0.4023\n",
      "Epoch 2, Batch 4680/5285, Loss: 0.5165\n",
      "Epoch 2, Batch 4690/5285, Loss: 0.2696\n",
      "Epoch 2, Batch 4700/5285, Loss: 0.5473\n",
      "Epoch 2, Batch 4710/5285, Loss: 0.3215\n",
      "Epoch 2, Batch 4720/5285, Loss: 0.7614\n",
      "Epoch 2, Batch 4730/5285, Loss: 0.2628\n",
      "Epoch 2, Batch 4740/5285, Loss: 0.5224\n",
      "Epoch 2, Batch 4750/5285, Loss: 0.8265\n",
      "Epoch 2, Batch 4760/5285, Loss: 0.5854\n",
      "Epoch 2, Batch 4770/5285, Loss: 0.5385\n",
      "Epoch 2, Batch 4780/5285, Loss: 0.2993\n",
      "Epoch 2, Batch 4790/5285, Loss: 0.3707\n",
      "Epoch 2, Batch 4800/5285, Loss: 0.5474\n",
      "Epoch 2, Batch 4810/5285, Loss: 0.2570\n",
      "Epoch 2, Batch 4820/5285, Loss: 0.5960\n",
      "Epoch 2, Batch 4830/5285, Loss: 1.0008\n",
      "Epoch 2, Batch 4840/5285, Loss: 0.3566\n",
      "Epoch 2, Batch 4850/5285, Loss: 0.4311\n",
      "Epoch 2, Batch 4860/5285, Loss: 0.4693\n",
      "Epoch 2, Batch 4870/5285, Loss: 0.2693\n",
      "Epoch 2, Batch 4880/5285, Loss: 0.9355\n",
      "Epoch 2, Batch 4890/5285, Loss: 0.6645\n",
      "Epoch 2, Batch 4900/5285, Loss: 0.2946\n",
      "Epoch 2, Batch 4910/5285, Loss: 0.3721\n",
      "Epoch 2, Batch 4920/5285, Loss: 0.4103\n",
      "Epoch 2, Batch 4930/5285, Loss: 0.4105\n",
      "Epoch 2, Batch 4940/5285, Loss: 0.5011\n",
      "Epoch 2, Batch 4950/5285, Loss: 0.3379\n",
      "Epoch 2, Batch 4960/5285, Loss: 0.6939\n",
      "Epoch 2, Batch 4970/5285, Loss: 0.6614\n",
      "Epoch 2, Batch 4980/5285, Loss: 0.3582\n",
      "Epoch 2, Batch 4990/5285, Loss: 0.1902\n",
      "Epoch 2, Batch 5000/5285, Loss: 0.2104\n",
      "Epoch 2, Batch 5010/5285, Loss: 0.9684\n",
      "Epoch 2, Batch 5020/5285, Loss: 0.7027\n",
      "Epoch 2, Batch 5030/5285, Loss: 0.3675\n",
      "Epoch 2, Batch 5040/5285, Loss: 0.3256\n",
      "Epoch 2, Batch 5050/5285, Loss: 0.5402\n",
      "Epoch 2, Batch 5060/5285, Loss: 0.3712\n",
      "Epoch 2, Batch 5070/5285, Loss: 0.2597\n",
      "Epoch 2, Batch 5080/5285, Loss: 0.1970\n",
      "Epoch 2, Batch 5090/5285, Loss: 0.3941\n",
      "Epoch 2, Batch 5100/5285, Loss: 0.2216\n",
      "Epoch 2, Batch 5110/5285, Loss: 0.3552\n",
      "Epoch 2, Batch 5120/5285, Loss: 0.2277\n",
      "Epoch 2, Batch 5130/5285, Loss: 0.4239\n",
      "Epoch 2, Batch 5140/5285, Loss: 0.7892\n",
      "Epoch 2, Batch 5150/5285, Loss: 0.7722\n",
      "Epoch 2, Batch 5160/5285, Loss: 0.6544\n",
      "Epoch 2, Batch 5170/5285, Loss: 0.5098\n",
      "Epoch 2, Batch 5180/5285, Loss: 0.5470\n",
      "Epoch 2, Batch 5190/5285, Loss: 0.6105\n",
      "Epoch 2, Batch 5200/5285, Loss: 0.6936\n",
      "Epoch 2, Batch 5210/5285, Loss: 0.5082\n",
      "Epoch 2, Batch 5220/5285, Loss: 0.6160\n",
      "Epoch 2, Batch 5230/5285, Loss: 0.2970\n",
      "Epoch 2, Batch 5240/5285, Loss: 0.5274\n",
      "Epoch 2, Batch 5250/5285, Loss: 0.1229\n",
      "Epoch 2, Batch 5260/5285, Loss: 0.3261\n",
      "Epoch 2, Batch 5270/5285, Loss: 0.3865\n",
      "Epoch 2, Batch 5280/5285, Loss: 0.2474\n",
      "Epoch 2, Average Loss: 0.4159, Accuracy: 81.85%\n",
      "Epoch 3, Batch 10/5285, Loss: 0.3296\n",
      "Epoch 3, Batch 20/5285, Loss: 0.5085\n",
      "Epoch 3, Batch 30/5285, Loss: 0.4559\n",
      "Epoch 3, Batch 40/5285, Loss: 0.4714\n",
      "Epoch 3, Batch 50/5285, Loss: 0.1916\n",
      "Epoch 3, Batch 60/5285, Loss: 0.1662\n",
      "Epoch 3, Batch 70/5285, Loss: 0.3231\n",
      "Epoch 3, Batch 80/5285, Loss: 0.3470\n",
      "Epoch 3, Batch 90/5285, Loss: 0.2606\n",
      "Epoch 3, Batch 100/5285, Loss: 0.5383\n",
      "Epoch 3, Batch 110/5285, Loss: 0.2380\n",
      "Epoch 3, Batch 120/5285, Loss: 0.4338\n",
      "Epoch 3, Batch 130/5285, Loss: 0.4642\n",
      "Epoch 3, Batch 140/5285, Loss: 0.4080\n",
      "Epoch 3, Batch 150/5285, Loss: 0.2785\n",
      "Epoch 3, Batch 160/5285, Loss: 0.4167\n",
      "Epoch 3, Batch 170/5285, Loss: 0.3924\n",
      "Epoch 3, Batch 180/5285, Loss: 0.5630\n",
      "Epoch 3, Batch 190/5285, Loss: 0.1085\n",
      "Epoch 3, Batch 200/5285, Loss: 0.6169\n",
      "Epoch 3, Batch 210/5285, Loss: 0.7222\n",
      "Epoch 3, Batch 220/5285, Loss: 0.2543\n",
      "Epoch 3, Batch 230/5285, Loss: 0.3100\n",
      "Epoch 3, Batch 240/5285, Loss: 0.3913\n",
      "Epoch 3, Batch 250/5285, Loss: 0.3530\n",
      "Epoch 3, Batch 260/5285, Loss: 0.3701\n",
      "Epoch 3, Batch 270/5285, Loss: 0.3137\n",
      "Epoch 3, Batch 280/5285, Loss: 0.4615\n",
      "Epoch 3, Batch 290/5285, Loss: 0.2235\n",
      "Epoch 3, Batch 300/5285, Loss: 0.8075\n",
      "Epoch 3, Batch 310/5285, Loss: 0.1797\n",
      "Epoch 3, Batch 320/5285, Loss: 0.3507\n",
      "Epoch 3, Batch 330/5285, Loss: 0.1390\n",
      "Epoch 3, Batch 340/5285, Loss: 0.4714\n",
      "Epoch 3, Batch 350/5285, Loss: 0.1937\n",
      "Epoch 3, Batch 360/5285, Loss: 0.1517\n",
      "Epoch 3, Batch 370/5285, Loss: 0.8294\n",
      "Epoch 3, Batch 380/5285, Loss: 0.2995\n",
      "Epoch 3, Batch 390/5285, Loss: 0.4055\n",
      "Epoch 3, Batch 400/5285, Loss: 0.1271\n",
      "Epoch 3, Batch 410/5285, Loss: 0.3111\n",
      "Epoch 3, Batch 420/5285, Loss: 0.4491\n",
      "Epoch 3, Batch 430/5285, Loss: 0.2784\n",
      "Epoch 3, Batch 440/5285, Loss: 0.3271\n",
      "Epoch 3, Batch 450/5285, Loss: 0.4827\n",
      "Epoch 3, Batch 460/5285, Loss: 0.2598\n",
      "Epoch 3, Batch 470/5285, Loss: 0.7871\n",
      "Epoch 3, Batch 480/5285, Loss: 0.1548\n",
      "Epoch 3, Batch 490/5285, Loss: 0.5168\n",
      "Epoch 3, Batch 500/5285, Loss: 0.3451\n",
      "Epoch 3, Batch 510/5285, Loss: 0.6150\n",
      "Epoch 3, Batch 520/5285, Loss: 0.3095\n",
      "Epoch 3, Batch 530/5285, Loss: 0.7285\n",
      "Epoch 3, Batch 540/5285, Loss: 0.1662\n",
      "Epoch 3, Batch 550/5285, Loss: 0.4288\n",
      "Epoch 3, Batch 560/5285, Loss: 0.3350\n",
      "Epoch 3, Batch 570/5285, Loss: 0.6941\n",
      "Epoch 3, Batch 580/5285, Loss: 0.5433\n",
      "Epoch 3, Batch 590/5285, Loss: 0.2763\n",
      "Epoch 3, Batch 600/5285, Loss: 0.4226\n",
      "Epoch 3, Batch 610/5285, Loss: 0.1921\n",
      "Epoch 3, Batch 620/5285, Loss: 0.5445\n",
      "Epoch 3, Batch 630/5285, Loss: 0.5058\n",
      "Epoch 3, Batch 640/5285, Loss: 0.1304\n",
      "Epoch 3, Batch 650/5285, Loss: 0.2816\n",
      "Epoch 3, Batch 660/5285, Loss: 0.4199\n",
      "Epoch 3, Batch 670/5285, Loss: 0.2481\n",
      "Epoch 3, Batch 680/5285, Loss: 0.2351\n",
      "Epoch 3, Batch 690/5285, Loss: 0.3871\n",
      "Epoch 3, Batch 700/5285, Loss: 0.6441\n",
      "Epoch 3, Batch 710/5285, Loss: 0.4778\n",
      "Epoch 3, Batch 720/5285, Loss: 0.3314\n",
      "Epoch 3, Batch 730/5285, Loss: 0.0726\n",
      "Epoch 3, Batch 740/5285, Loss: 0.3829\n",
      "Epoch 3, Batch 750/5285, Loss: 0.2900\n",
      "Epoch 3, Batch 760/5285, Loss: 0.1239\n",
      "Epoch 3, Batch 770/5285, Loss: 0.3193\n",
      "Epoch 3, Batch 780/5285, Loss: 0.5480\n",
      "Epoch 3, Batch 790/5285, Loss: 0.3372\n",
      "Epoch 3, Batch 800/5285, Loss: 0.6037\n",
      "Epoch 3, Batch 810/5285, Loss: 0.6919\n",
      "Epoch 3, Batch 820/5285, Loss: 0.1534\n",
      "Epoch 3, Batch 830/5285, Loss: 0.5690\n",
      "Epoch 3, Batch 840/5285, Loss: 0.1882\n",
      "Epoch 3, Batch 850/5285, Loss: 0.3204\n",
      "Epoch 3, Batch 860/5285, Loss: 0.1440\n",
      "Epoch 3, Batch 870/5285, Loss: 0.4374\n",
      "Epoch 3, Batch 880/5285, Loss: 1.2717\n",
      "Epoch 3, Batch 890/5285, Loss: 0.4778\n",
      "Epoch 3, Batch 900/5285, Loss: 0.1995\n",
      "Epoch 3, Batch 910/5285, Loss: 0.2900\n",
      "Epoch 3, Batch 920/5285, Loss: 0.6752\n",
      "Epoch 3, Batch 930/5285, Loss: 0.7152\n",
      "Epoch 3, Batch 940/5285, Loss: 0.3909\n",
      "Epoch 3, Batch 950/5285, Loss: 0.4348\n",
      "Epoch 3, Batch 960/5285, Loss: 0.4691\n",
      "Epoch 3, Batch 970/5285, Loss: 0.5115\n",
      "Epoch 3, Batch 980/5285, Loss: 0.2253\n",
      "Epoch 3, Batch 990/5285, Loss: 0.6669\n",
      "Epoch 3, Batch 1000/5285, Loss: 0.1365\n",
      "Epoch 3, Batch 1010/5285, Loss: 0.8839\n",
      "Epoch 3, Batch 1020/5285, Loss: 0.8580\n",
      "Epoch 3, Batch 1030/5285, Loss: 0.0793\n",
      "Epoch 3, Batch 1040/5285, Loss: 0.6858\n",
      "Epoch 3, Batch 1050/5285, Loss: 0.1885\n",
      "Epoch 3, Batch 1060/5285, Loss: 0.4020\n",
      "Epoch 3, Batch 1070/5285, Loss: 0.1720\n",
      "Epoch 3, Batch 1080/5285, Loss: 0.2611\n",
      "Epoch 3, Batch 1090/5285, Loss: 0.2096\n",
      "Epoch 3, Batch 1100/5285, Loss: 0.2497\n",
      "Epoch 3, Batch 1110/5285, Loss: 0.2243\n",
      "Epoch 3, Batch 1120/5285, Loss: 0.2178\n",
      "Epoch 3, Batch 1130/5285, Loss: 0.4791\n",
      "Epoch 3, Batch 1140/5285, Loss: 0.5205\n",
      "Epoch 3, Batch 1150/5285, Loss: 0.6554\n",
      "Epoch 3, Batch 1160/5285, Loss: 0.2729\n",
      "Epoch 3, Batch 1170/5285, Loss: 0.3499\n",
      "Epoch 3, Batch 1180/5285, Loss: 0.2037\n",
      "Epoch 3, Batch 1190/5285, Loss: 0.2157\n",
      "Epoch 3, Batch 1200/5285, Loss: 0.1466\n",
      "Epoch 3, Batch 1210/5285, Loss: 0.5228\n",
      "Epoch 3, Batch 1220/5285, Loss: 0.1998\n",
      "Epoch 3, Batch 1230/5285, Loss: 0.5581\n",
      "Epoch 3, Batch 1240/5285, Loss: 0.2687\n",
      "Epoch 3, Batch 1250/5285, Loss: 0.2723\n",
      "Epoch 3, Batch 1260/5285, Loss: 0.3764\n",
      "Epoch 3, Batch 1270/5285, Loss: 0.3177\n",
      "Epoch 3, Batch 1280/5285, Loss: 0.9468\n",
      "Epoch 3, Batch 1290/5285, Loss: 0.2602\n",
      "Epoch 3, Batch 1300/5285, Loss: 0.2293\n",
      "Epoch 3, Batch 1310/5285, Loss: 0.2735\n",
      "Epoch 3, Batch 1320/5285, Loss: 0.3224\n",
      "Epoch 3, Batch 1330/5285, Loss: 0.3542\n",
      "Epoch 3, Batch 1340/5285, Loss: 0.4095\n",
      "Epoch 3, Batch 1350/5285, Loss: 0.2475\n",
      "Epoch 3, Batch 1360/5285, Loss: 0.5339\n",
      "Epoch 3, Batch 1370/5285, Loss: 0.4559\n",
      "Epoch 3, Batch 1380/5285, Loss: 0.2207\n",
      "Epoch 3, Batch 1390/5285, Loss: 0.1756\n",
      "Epoch 3, Batch 1400/5285, Loss: 0.3402\n",
      "Epoch 3, Batch 1410/5285, Loss: 0.5069\n",
      "Epoch 3, Batch 1420/5285, Loss: 0.4381\n",
      "Epoch 3, Batch 1430/5285, Loss: 0.6624\n",
      "Epoch 3, Batch 1440/5285, Loss: 0.2978\n",
      "Epoch 3, Batch 1450/5285, Loss: 0.3279\n",
      "Epoch 3, Batch 1460/5285, Loss: 0.3118\n",
      "Epoch 3, Batch 1470/5285, Loss: 0.4412\n",
      "Epoch 3, Batch 1480/5285, Loss: 0.3306\n",
      "Epoch 3, Batch 1490/5285, Loss: 0.3297\n",
      "Epoch 3, Batch 1500/5285, Loss: 0.3722\n",
      "Epoch 3, Batch 1510/5285, Loss: 0.2874\n",
      "Epoch 3, Batch 1520/5285, Loss: 0.3902\n",
      "Epoch 3, Batch 1530/5285, Loss: 0.1627\n",
      "Epoch 3, Batch 1540/5285, Loss: 0.1146\n",
      "Epoch 3, Batch 1550/5285, Loss: 0.2585\n",
      "Epoch 3, Batch 1560/5285, Loss: 0.2049\n",
      "Epoch 3, Batch 1570/5285, Loss: 0.4648\n",
      "Epoch 3, Batch 1580/5285, Loss: 0.3964\n",
      "Epoch 3, Batch 1590/5285, Loss: 0.2516\n",
      "Epoch 3, Batch 1600/5285, Loss: 0.4576\n",
      "Epoch 3, Batch 1610/5285, Loss: 0.3644\n",
      "Epoch 3, Batch 1620/5285, Loss: 0.2241\n",
      "Epoch 3, Batch 1630/5285, Loss: 0.4630\n",
      "Epoch 3, Batch 1640/5285, Loss: 0.2723\n",
      "Epoch 3, Batch 1650/5285, Loss: 0.5304\n",
      "Epoch 3, Batch 1660/5285, Loss: 0.3240\n",
      "Epoch 3, Batch 1670/5285, Loss: 0.2902\n",
      "Epoch 3, Batch 1680/5285, Loss: 0.4604\n",
      "Epoch 3, Batch 1690/5285, Loss: 0.3388\n",
      "Epoch 3, Batch 1700/5285, Loss: 0.2543\n",
      "Epoch 3, Batch 1710/5285, Loss: 0.5225\n",
      "Epoch 3, Batch 1720/5285, Loss: 0.1159\n",
      "Epoch 3, Batch 1730/5285, Loss: 0.4509\n",
      "Epoch 3, Batch 1740/5285, Loss: 0.4612\n",
      "Epoch 3, Batch 1750/5285, Loss: 0.4396\n",
      "Epoch 3, Batch 1760/5285, Loss: 0.0391\n",
      "Epoch 3, Batch 1770/5285, Loss: 0.0747\n",
      "Epoch 3, Batch 1780/5285, Loss: 0.3899\n",
      "Epoch 3, Batch 1790/5285, Loss: 0.6271\n",
      "Epoch 3, Batch 1800/5285, Loss: 0.2625\n",
      "Epoch 3, Batch 1810/5285, Loss: 0.3991\n",
      "Epoch 3, Batch 1820/5285, Loss: 0.3606\n",
      "Epoch 3, Batch 1830/5285, Loss: 0.5402\n",
      "Epoch 3, Batch 1840/5285, Loss: 0.2590\n",
      "Epoch 3, Batch 1850/5285, Loss: 0.7568\n",
      "Epoch 3, Batch 1860/5285, Loss: 0.3885\n",
      "Epoch 3, Batch 1870/5285, Loss: 0.7572\n",
      "Epoch 3, Batch 1880/5285, Loss: 0.3678\n",
      "Epoch 3, Batch 1890/5285, Loss: 0.3933\n",
      "Epoch 3, Batch 1900/5285, Loss: 0.3381\n",
      "Epoch 3, Batch 1910/5285, Loss: 0.3932\n",
      "Epoch 3, Batch 1920/5285, Loss: 0.1927\n",
      "Epoch 3, Batch 1930/5285, Loss: 0.2466\n",
      "Epoch 3, Batch 1940/5285, Loss: 0.3343\n",
      "Epoch 3, Batch 1950/5285, Loss: 0.4372\n",
      "Epoch 3, Batch 1960/5285, Loss: 0.3846\n",
      "Epoch 3, Batch 1970/5285, Loss: 0.1149\n",
      "Epoch 3, Batch 1980/5285, Loss: 0.4414\n",
      "Epoch 3, Batch 1990/5285, Loss: 0.3705\n",
      "Epoch 3, Batch 2000/5285, Loss: 0.3254\n",
      "Epoch 3, Batch 2010/5285, Loss: 0.4043\n",
      "Epoch 3, Batch 2020/5285, Loss: 0.5416\n",
      "Epoch 3, Batch 2030/5285, Loss: 0.4637\n",
      "Epoch 3, Batch 2040/5285, Loss: 0.8165\n",
      "Epoch 3, Batch 2050/5285, Loss: 0.4743\n",
      "Epoch 3, Batch 2060/5285, Loss: 0.3258\n",
      "Epoch 3, Batch 2070/5285, Loss: 0.4996\n",
      "Epoch 3, Batch 2080/5285, Loss: 0.3866\n",
      "Epoch 3, Batch 2090/5285, Loss: 0.4501\n",
      "Epoch 3, Batch 2100/5285, Loss: 0.4425\n",
      "Epoch 3, Batch 2110/5285, Loss: 0.3886\n",
      "Epoch 3, Batch 2120/5285, Loss: 0.3865\n",
      "Epoch 3, Batch 2130/5285, Loss: 0.5076\n",
      "Epoch 3, Batch 2140/5285, Loss: 0.4371\n",
      "Epoch 3, Batch 2150/5285, Loss: 0.4085\n",
      "Epoch 3, Batch 2160/5285, Loss: 0.2894\n",
      "Epoch 3, Batch 2170/5285, Loss: 0.1736\n",
      "Epoch 3, Batch 2180/5285, Loss: 0.4020\n",
      "Epoch 3, Batch 2190/5285, Loss: 0.2579\n",
      "Epoch 3, Batch 2200/5285, Loss: 0.3916\n",
      "Epoch 3, Batch 2210/5285, Loss: 0.4504\n",
      "Epoch 3, Batch 2220/5285, Loss: 0.3238\n",
      "Epoch 3, Batch 2230/5285, Loss: 0.2175\n",
      "Epoch 3, Batch 2240/5285, Loss: 0.3620\n",
      "Epoch 3, Batch 2250/5285, Loss: 0.7375\n",
      "Epoch 3, Batch 2260/5285, Loss: 0.2822\n",
      "Epoch 3, Batch 2270/5285, Loss: 0.2498\n",
      "Epoch 3, Batch 2280/5285, Loss: 0.4785\n",
      "Epoch 3, Batch 2290/5285, Loss: 0.6304\n",
      "Epoch 3, Batch 2300/5285, Loss: 0.1854\n",
      "Epoch 3, Batch 2310/5285, Loss: 0.5296\n",
      "Epoch 3, Batch 2320/5285, Loss: 0.6266\n",
      "Epoch 3, Batch 2330/5285, Loss: 0.6793\n",
      "Epoch 3, Batch 2340/5285, Loss: 0.4119\n",
      "Epoch 3, Batch 2350/5285, Loss: 0.5553\n",
      "Epoch 3, Batch 2360/5285, Loss: 0.1173\n",
      "Epoch 3, Batch 2370/5285, Loss: 0.5146\n",
      "Epoch 3, Batch 2380/5285, Loss: 0.3958\n",
      "Epoch 3, Batch 2390/5285, Loss: 0.2153\n",
      "Epoch 3, Batch 2400/5285, Loss: 0.4035\n",
      "Epoch 3, Batch 2410/5285, Loss: 0.8480\n",
      "Epoch 3, Batch 2420/5285, Loss: 0.1786\n",
      "Epoch 3, Batch 2430/5285, Loss: 0.2706\n",
      "Epoch 3, Batch 2440/5285, Loss: 0.7670\n",
      "Epoch 3, Batch 2450/5285, Loss: 0.3904\n",
      "Epoch 3, Batch 2460/5285, Loss: 0.5192\n",
      "Epoch 3, Batch 2470/5285, Loss: 0.2345\n",
      "Epoch 3, Batch 2480/5285, Loss: 0.4148\n",
      "Epoch 3, Batch 2490/5285, Loss: 0.4468\n",
      "Epoch 3, Batch 2500/5285, Loss: 0.2164\n",
      "Epoch 3, Batch 2510/5285, Loss: 0.4758\n",
      "Epoch 3, Batch 2520/5285, Loss: 0.4813\n",
      "Epoch 3, Batch 2530/5285, Loss: 0.4106\n",
      "Epoch 3, Batch 2540/5285, Loss: 0.5240\n",
      "Epoch 3, Batch 2550/5285, Loss: 0.2488\n",
      "Epoch 3, Batch 2560/5285, Loss: 0.3035\n",
      "Epoch 3, Batch 2570/5285, Loss: 0.2304\n",
      "Epoch 3, Batch 2580/5285, Loss: 0.2893\n",
      "Epoch 3, Batch 2590/5285, Loss: 0.3998\n",
      "Epoch 3, Batch 2600/5285, Loss: 0.2138\n",
      "Epoch 3, Batch 2610/5285, Loss: 0.7892\n",
      "Epoch 3, Batch 2620/5285, Loss: 0.3835\n",
      "Epoch 3, Batch 2630/5285, Loss: 0.4071\n",
      "Epoch 3, Batch 2640/5285, Loss: 0.3005\n",
      "Epoch 3, Batch 2650/5285, Loss: 0.3484\n",
      "Epoch 3, Batch 2660/5285, Loss: 0.3664\n",
      "Epoch 3, Batch 2670/5285, Loss: 0.3198\n",
      "Epoch 3, Batch 2680/5285, Loss: 0.3476\n",
      "Epoch 3, Batch 2690/5285, Loss: 0.6088\n",
      "Epoch 3, Batch 2700/5285, Loss: 0.7058\n",
      "Epoch 3, Batch 2710/5285, Loss: 1.1339\n",
      "Epoch 3, Batch 2720/5285, Loss: 0.2282\n",
      "Epoch 3, Batch 2730/5285, Loss: 0.3947\n",
      "Epoch 3, Batch 2740/5285, Loss: 0.5134\n",
      "Epoch 3, Batch 2750/5285, Loss: 0.2950\n",
      "Epoch 3, Batch 2760/5285, Loss: 0.2203\n",
      "Epoch 3, Batch 2770/5285, Loss: 0.2583\n",
      "Epoch 3, Batch 2780/5285, Loss: 0.2642\n",
      "Epoch 3, Batch 2790/5285, Loss: 0.4825\n",
      "Epoch 3, Batch 2800/5285, Loss: 0.3625\n",
      "Epoch 3, Batch 2810/5285, Loss: 0.3697\n",
      "Epoch 3, Batch 2820/5285, Loss: 0.3754\n",
      "Epoch 3, Batch 2830/5285, Loss: 0.3902\n",
      "Epoch 3, Batch 2840/5285, Loss: 0.3172\n",
      "Epoch 3, Batch 2850/5285, Loss: 0.3501\n",
      "Epoch 3, Batch 2860/5285, Loss: 0.4225\n",
      "Epoch 3, Batch 2870/5285, Loss: 0.3214\n",
      "Epoch 3, Batch 2880/5285, Loss: 0.3748\n",
      "Epoch 3, Batch 2890/5285, Loss: 0.7445\n",
      "Epoch 3, Batch 2900/5285, Loss: 0.2324\n",
      "Epoch 3, Batch 2910/5285, Loss: 0.3985\n",
      "Epoch 3, Batch 2920/5285, Loss: 0.1129\n",
      "Epoch 3, Batch 2930/5285, Loss: 0.1609\n",
      "Epoch 3, Batch 2940/5285, Loss: 0.1555\n",
      "Epoch 3, Batch 2950/5285, Loss: 0.3273\n",
      "Epoch 3, Batch 2960/5285, Loss: 0.2602\n",
      "Epoch 3, Batch 2970/5285, Loss: 0.4797\n",
      "Epoch 3, Batch 2980/5285, Loss: 0.6393\n",
      "Epoch 3, Batch 2990/5285, Loss: 0.7860\n",
      "Epoch 3, Batch 3000/5285, Loss: 0.2953\n",
      "Epoch 3, Batch 3010/5285, Loss: 0.2517\n",
      "Epoch 3, Batch 3020/5285, Loss: 0.4077\n",
      "Epoch 3, Batch 3030/5285, Loss: 0.4109\n",
      "Epoch 3, Batch 3040/5285, Loss: 0.1964\n",
      "Epoch 3, Batch 3050/5285, Loss: 0.3523\n",
      "Epoch 3, Batch 3060/5285, Loss: 0.1525\n",
      "Epoch 3, Batch 3070/5285, Loss: 0.3160\n",
      "Epoch 3, Batch 3080/5285, Loss: 0.3841\n",
      "Epoch 3, Batch 3090/5285, Loss: 0.4880\n",
      "Epoch 3, Batch 3100/5285, Loss: 0.5555\n",
      "Epoch 3, Batch 3110/5285, Loss: 0.4029\n",
      "Epoch 3, Batch 3120/5285, Loss: 0.2692\n",
      "Epoch 3, Batch 3130/5285, Loss: 0.5813\n",
      "Epoch 3, Batch 3140/5285, Loss: 0.1577\n",
      "Epoch 3, Batch 3150/5285, Loss: 0.6143\n",
      "Epoch 3, Batch 3160/5285, Loss: 0.5104\n",
      "Epoch 3, Batch 3170/5285, Loss: 0.1560\n",
      "Epoch 3, Batch 3180/5285, Loss: 0.2617\n",
      "Epoch 3, Batch 3190/5285, Loss: 0.4304\n",
      "Epoch 3, Batch 3200/5285, Loss: 0.2543\n",
      "Epoch 3, Batch 3210/5285, Loss: 0.3818\n",
      "Epoch 3, Batch 3220/5285, Loss: 0.2260\n",
      "Epoch 3, Batch 3230/5285, Loss: 0.6042\n",
      "Epoch 3, Batch 3240/5285, Loss: 0.0850\n",
      "Epoch 3, Batch 3250/5285, Loss: 0.6468\n",
      "Epoch 3, Batch 3260/5285, Loss: 0.2913\n",
      "Epoch 3, Batch 3270/5285, Loss: 0.4847\n",
      "Epoch 3, Batch 3280/5285, Loss: 0.3774\n",
      "Epoch 3, Batch 3290/5285, Loss: 0.5398\n",
      "Epoch 3, Batch 3300/5285, Loss: 0.4392\n",
      "Epoch 3, Batch 3310/5285, Loss: 0.7205\n",
      "Epoch 3, Batch 3320/5285, Loss: 0.5602\n",
      "Epoch 3, Batch 3330/5285, Loss: 0.3298\n",
      "Epoch 3, Batch 3340/5285, Loss: 0.2624\n",
      "Epoch 3, Batch 3350/5285, Loss: 0.4115\n",
      "Epoch 3, Batch 3360/5285, Loss: 0.4872\n",
      "Epoch 3, Batch 3370/5285, Loss: 0.2852\n",
      "Epoch 3, Batch 3380/5285, Loss: 0.1848\n",
      "Epoch 3, Batch 3390/5285, Loss: 0.4323\n",
      "Epoch 3, Batch 3400/5285, Loss: 0.4893\n",
      "Epoch 3, Batch 3410/5285, Loss: 0.3356\n",
      "Epoch 3, Batch 3420/5285, Loss: 0.2305\n",
      "Epoch 3, Batch 3430/5285, Loss: 0.2966\n",
      "Epoch 3, Batch 3440/5285, Loss: 0.2198\n",
      "Epoch 3, Batch 3450/5285, Loss: 0.4656\n",
      "Epoch 3, Batch 3460/5285, Loss: 0.3729\n",
      "Epoch 3, Batch 3470/5285, Loss: 0.4329\n",
      "Epoch 3, Batch 3480/5285, Loss: 0.1662\n",
      "Epoch 3, Batch 3490/5285, Loss: 0.1890\n",
      "Epoch 3, Batch 3500/5285, Loss: 0.5852\n",
      "Epoch 3, Batch 3510/5285, Loss: 0.3545\n",
      "Epoch 3, Batch 3520/5285, Loss: 0.2723\n",
      "Epoch 3, Batch 3530/5285, Loss: 0.6093\n",
      "Epoch 3, Batch 3540/5285, Loss: 0.4501\n",
      "Epoch 3, Batch 3550/5285, Loss: 0.1412\n",
      "Epoch 3, Batch 3560/5285, Loss: 0.4718\n",
      "Epoch 3, Batch 3570/5285, Loss: 0.4971\n",
      "Epoch 3, Batch 3580/5285, Loss: 0.5483\n",
      "Epoch 3, Batch 3590/5285, Loss: 0.3385\n",
      "Epoch 3, Batch 3600/5285, Loss: 0.1453\n",
      "Epoch 3, Batch 3610/5285, Loss: 0.5794\n",
      "Epoch 3, Batch 3620/5285, Loss: 0.4859\n",
      "Epoch 3, Batch 3630/5285, Loss: 0.4975\n",
      "Epoch 3, Batch 3640/5285, Loss: 0.2877\n",
      "Epoch 3, Batch 3650/5285, Loss: 0.2982\n",
      "Epoch 3, Batch 3660/5285, Loss: 0.6594\n",
      "Epoch 3, Batch 3670/5285, Loss: 0.3463\n",
      "Epoch 3, Batch 3680/5285, Loss: 0.3679\n",
      "Epoch 3, Batch 3690/5285, Loss: 0.2190\n",
      "Epoch 3, Batch 3700/5285, Loss: 0.8354\n",
      "Epoch 3, Batch 3710/5285, Loss: 0.6004\n",
      "Epoch 3, Batch 3720/5285, Loss: 0.3066\n",
      "Epoch 3, Batch 3730/5285, Loss: 0.5647\n",
      "Epoch 3, Batch 3740/5285, Loss: 0.4457\n",
      "Epoch 3, Batch 3750/5285, Loss: 0.5961\n",
      "Epoch 3, Batch 3760/5285, Loss: 0.7280\n",
      "Epoch 3, Batch 3770/5285, Loss: 0.4313\n",
      "Epoch 3, Batch 3780/5285, Loss: 0.5019\n",
      "Epoch 3, Batch 3790/5285, Loss: 0.4365\n",
      "Epoch 3, Batch 3800/5285, Loss: 0.3531\n",
      "Epoch 3, Batch 3810/5285, Loss: 0.3938\n",
      "Epoch 3, Batch 3820/5285, Loss: 0.4759\n",
      "Epoch 3, Batch 3830/5285, Loss: 0.3589\n",
      "Epoch 3, Batch 3840/5285, Loss: 0.3377\n",
      "Epoch 3, Batch 3850/5285, Loss: 0.2905\n",
      "Epoch 3, Batch 3860/5285, Loss: 0.3465\n",
      "Epoch 3, Batch 3870/5285, Loss: 0.2567\n",
      "Epoch 3, Batch 3880/5285, Loss: 0.4146\n",
      "Epoch 3, Batch 3890/5285, Loss: 0.2325\n",
      "Epoch 3, Batch 3900/5285, Loss: 0.2283\n",
      "Epoch 3, Batch 3910/5285, Loss: 0.3113\n",
      "Epoch 3, Batch 3920/5285, Loss: 0.2018\n",
      "Epoch 3, Batch 3930/5285, Loss: 0.3193\n",
      "Epoch 3, Batch 3940/5285, Loss: 0.3941\n",
      "Epoch 3, Batch 3950/5285, Loss: 0.3384\n",
      "Epoch 3, Batch 3960/5285, Loss: 0.2308\n",
      "Epoch 3, Batch 3970/5285, Loss: 0.2123\n",
      "Epoch 3, Batch 3980/5285, Loss: 0.5296\n",
      "Epoch 3, Batch 3990/5285, Loss: 0.2756\n",
      "Epoch 3, Batch 4000/5285, Loss: 0.8437\n",
      "Epoch 3, Batch 4010/5285, Loss: 0.2504\n",
      "Epoch 3, Batch 4020/5285, Loss: 0.5864\n",
      "Epoch 3, Batch 4030/5285, Loss: 0.2360\n",
      "Epoch 3, Batch 4040/5285, Loss: 0.3934\n",
      "Epoch 3, Batch 4050/5285, Loss: 0.8557\n",
      "Epoch 3, Batch 4060/5285, Loss: 0.4558\n",
      "Epoch 3, Batch 4070/5285, Loss: 0.2951\n",
      "Epoch 3, Batch 4080/5285, Loss: 0.5059\n",
      "Epoch 3, Batch 4090/5285, Loss: 0.3665\n",
      "Epoch 3, Batch 4100/5285, Loss: 0.7995\n",
      "Epoch 3, Batch 4110/5285, Loss: 0.4531\n",
      "Epoch 3, Batch 4120/5285, Loss: 0.7027\n",
      "Epoch 3, Batch 4130/5285, Loss: 0.2115\n",
      "Epoch 3, Batch 4140/5285, Loss: 0.2111\n",
      "Epoch 3, Batch 4150/5285, Loss: 0.4051\n",
      "Epoch 3, Batch 4160/5285, Loss: 0.1624\n",
      "Epoch 3, Batch 4170/5285, Loss: 0.2491\n",
      "Epoch 3, Batch 4180/5285, Loss: 0.3634\n",
      "Epoch 3, Batch 4190/5285, Loss: 0.3661\n",
      "Epoch 3, Batch 4200/5285, Loss: 0.4119\n",
      "Epoch 3, Batch 4210/5285, Loss: 0.3741\n",
      "Epoch 3, Batch 4220/5285, Loss: 0.1909\n",
      "Epoch 3, Batch 4230/5285, Loss: 0.4361\n",
      "Epoch 3, Batch 4240/5285, Loss: 0.2513\n",
      "Epoch 3, Batch 4250/5285, Loss: 0.1896\n",
      "Epoch 3, Batch 4260/5285, Loss: 0.1341\n",
      "Epoch 3, Batch 4270/5285, Loss: 0.1210\n",
      "Epoch 3, Batch 4280/5285, Loss: 0.5561\n",
      "Epoch 3, Batch 4290/5285, Loss: 0.5463\n",
      "Epoch 3, Batch 4300/5285, Loss: 0.4923\n",
      "Epoch 3, Batch 4310/5285, Loss: 0.3941\n",
      "Epoch 3, Batch 4320/5285, Loss: 0.2386\n",
      "Epoch 3, Batch 4330/5285, Loss: 0.6392\n",
      "Epoch 3, Batch 4340/5285, Loss: 0.4682\n",
      "Epoch 3, Batch 4350/5285, Loss: 0.1728\n",
      "Epoch 3, Batch 4360/5285, Loss: 0.4908\n",
      "Epoch 3, Batch 4370/5285, Loss: 0.5099\n",
      "Epoch 3, Batch 4380/5285, Loss: 0.6105\n",
      "Epoch 3, Batch 4390/5285, Loss: 0.4091\n",
      "Epoch 3, Batch 4400/5285, Loss: 0.3665\n",
      "Epoch 3, Batch 4410/5285, Loss: 0.4451\n",
      "Epoch 3, Batch 4420/5285, Loss: 0.3264\n",
      "Epoch 3, Batch 4430/5285, Loss: 0.1377\n",
      "Epoch 3, Batch 4440/5285, Loss: 0.5663\n",
      "Epoch 3, Batch 4450/5285, Loss: 0.3920\n",
      "Epoch 3, Batch 4460/5285, Loss: 0.8500\n",
      "Epoch 3, Batch 4470/5285, Loss: 0.3464\n",
      "Epoch 3, Batch 4480/5285, Loss: 0.2142\n",
      "Epoch 3, Batch 4490/5285, Loss: 0.2670\n",
      "Epoch 3, Batch 4500/5285, Loss: 0.2124\n",
      "Epoch 3, Batch 4510/5285, Loss: 0.6735\n",
      "Epoch 3, Batch 4520/5285, Loss: 0.1982\n",
      "Epoch 3, Batch 4530/5285, Loss: 0.4052\n",
      "Epoch 3, Batch 4540/5285, Loss: 0.3882\n",
      "Epoch 3, Batch 4550/5285, Loss: 0.3995\n",
      "Epoch 3, Batch 4560/5285, Loss: 0.6245\n",
      "Epoch 3, Batch 4570/5285, Loss: 0.4273\n",
      "Epoch 3, Batch 4580/5285, Loss: 0.7587\n",
      "Epoch 3, Batch 4590/5285, Loss: 0.6171\n",
      "Epoch 3, Batch 4600/5285, Loss: 0.2113\n",
      "Epoch 3, Batch 4610/5285, Loss: 0.3582\n",
      "Epoch 3, Batch 4620/5285, Loss: 0.2717\n",
      "Epoch 3, Batch 4630/5285, Loss: 0.2796\n",
      "Epoch 3, Batch 4640/5285, Loss: 0.1574\n",
      "Epoch 3, Batch 4650/5285, Loss: 0.3702\n",
      "Epoch 3, Batch 4660/5285, Loss: 0.3769\n",
      "Epoch 3, Batch 4670/5285, Loss: 0.2762\n",
      "Epoch 3, Batch 4680/5285, Loss: 0.6757\n",
      "Epoch 3, Batch 4690/5285, Loss: 0.3842\n",
      "Epoch 3, Batch 4700/5285, Loss: 0.4029\n",
      "Epoch 3, Batch 4710/5285, Loss: 0.5517\n",
      "Epoch 3, Batch 4720/5285, Loss: 0.1786\n",
      "Epoch 3, Batch 4730/5285, Loss: 0.3141\n",
      "Epoch 3, Batch 4740/5285, Loss: 0.4236\n",
      "Epoch 3, Batch 4750/5285, Loss: 0.2206\n",
      "Epoch 3, Batch 4760/5285, Loss: 0.3229\n",
      "Epoch 3, Batch 4770/5285, Loss: 0.2522\n",
      "Epoch 3, Batch 4780/5285, Loss: 0.3378\n",
      "Epoch 3, Batch 4790/5285, Loss: 0.5604\n",
      "Epoch 3, Batch 4800/5285, Loss: 0.3861\n",
      "Epoch 3, Batch 4810/5285, Loss: 0.7280\n",
      "Epoch 3, Batch 4820/5285, Loss: 0.4491\n",
      "Epoch 3, Batch 4830/5285, Loss: 0.3280\n",
      "Epoch 3, Batch 4840/5285, Loss: 0.3050\n",
      "Epoch 3, Batch 4850/5285, Loss: 0.1776\n",
      "Epoch 3, Batch 4860/5285, Loss: 0.2483\n",
      "Epoch 3, Batch 4870/5285, Loss: 0.4107\n",
      "Epoch 3, Batch 4880/5285, Loss: 0.5384\n",
      "Epoch 3, Batch 4890/5285, Loss: 0.3081\n",
      "Epoch 3, Batch 4900/5285, Loss: 0.2823\n",
      "Epoch 3, Batch 4910/5285, Loss: 0.3157\n",
      "Epoch 3, Batch 4920/5285, Loss: 0.1438\n",
      "Epoch 3, Batch 4930/5285, Loss: 0.5860\n",
      "Epoch 3, Batch 4940/5285, Loss: 0.7069\n",
      "Epoch 3, Batch 4950/5285, Loss: 0.6086\n",
      "Epoch 3, Batch 4960/5285, Loss: 0.2497\n",
      "Epoch 3, Batch 4970/5285, Loss: 0.5974\n",
      "Epoch 3, Batch 4980/5285, Loss: 0.3938\n",
      "Epoch 3, Batch 4990/5285, Loss: 0.3962\n",
      "Epoch 3, Batch 5000/5285, Loss: 0.4293\n",
      "Epoch 3, Batch 5010/5285, Loss: 0.2959\n",
      "Epoch 3, Batch 5020/5285, Loss: 0.2821\n",
      "Epoch 3, Batch 5030/5285, Loss: 0.3716\n",
      "Epoch 3, Batch 5040/5285, Loss: 0.2910\n",
      "Epoch 3, Batch 5050/5285, Loss: 0.4874\n",
      "Epoch 3, Batch 5060/5285, Loss: 0.3789\n",
      "Epoch 3, Batch 5070/5285, Loss: 0.2311\n",
      "Epoch 3, Batch 5080/5285, Loss: 0.3614\n",
      "Epoch 3, Batch 5090/5285, Loss: 0.2733\n",
      "Epoch 3, Batch 5100/5285, Loss: 0.2213\n",
      "Epoch 3, Batch 5110/5285, Loss: 0.2986\n",
      "Epoch 3, Batch 5120/5285, Loss: 0.3343\n",
      "Epoch 3, Batch 5130/5285, Loss: 0.3511\n",
      "Epoch 3, Batch 5140/5285, Loss: 0.6019\n",
      "Epoch 3, Batch 5150/5285, Loss: 0.3783\n",
      "Epoch 3, Batch 5160/5285, Loss: 0.2993\n",
      "Epoch 3, Batch 5170/5285, Loss: 0.2551\n",
      "Epoch 3, Batch 5180/5285, Loss: 0.6729\n",
      "Epoch 3, Batch 5190/5285, Loss: 0.4502\n",
      "Epoch 3, Batch 5200/5285, Loss: 0.4959\n",
      "Epoch 3, Batch 5210/5285, Loss: 0.3771\n",
      "Epoch 3, Batch 5220/5285, Loss: 0.3699\n",
      "Epoch 3, Batch 5230/5285, Loss: 0.4876\n",
      "Epoch 3, Batch 5240/5285, Loss: 0.5233\n",
      "Epoch 3, Batch 5250/5285, Loss: 0.3651\n",
      "Epoch 3, Batch 5260/5285, Loss: 0.4356\n",
      "Epoch 3, Batch 5270/5285, Loss: 0.2995\n",
      "Epoch 3, Batch 5280/5285, Loss: 0.5040\n",
      "Epoch 3, Average Loss: 0.3900, Accuracy: 83.15%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Select device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(logits, dim=1)  # Get the predicted class\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Print progress every 10 batches\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Batch {i + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Epoch metrics\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_accuracy = (correct_predictions / total_samples) * 100  # Convert to percentage\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function evaluates a trained model on a test dataset and reports its performance.\n",
    "\n",
    "1. **Function Arguments**:  \n",
    "   - **`model`**: The pre-trained and fine-tuned model to evaluate.  \n",
    "   - **`test_loader`**: The DataLoader object for the test dataset, which provides batches of test data.  \n",
    "   - **`label_mapping`**: A dictionary mapping internal model labels to the original dataset labels.\n",
    "\n",
    "2. **Evaluation Mode**:  \n",
    "   - The model is set to evaluation mode (`model.eval()`), which deactivates layers like dropout to ensure consistent predictions.\n",
    "\n",
    "3. **Prediction and True Label Collection**:  \n",
    "   - Iterates through the test dataset using the `test_loader`.\n",
    "   - Moves inputs (`input_ids`, `attention_mask`, and `labels`) to the selected device (CPU/GPU).\n",
    "   - Performs forward passes to generate logits and uses `torch.max` to get predicted classes.\n",
    "   - Appends predictions and true labels to separate lists for further analysis.\n",
    "\n",
    "4. **Reverse Mapping of Labels**:  \n",
    "   - Converts predictions and true labels back to their original label format using `label_mapping`.\n",
    "\n",
    "5. **Metrics Calculation**:  \n",
    "   - **Accuracy**: The percentage of correctly predicted samples. Calculated using `accuracy_score`.\n",
    "   - **Classification Report**: A detailed breakdown of metrics such as precision, recall, and F1-score for each class, using `classification_report`.\n",
    "\n",
    "6. **Outputs**:  \n",
    "   - Prints accuracy and the classification report.\n",
    "   - Returns lists of mapped predictions and true labels for further use.\n",
    "\n",
    "### Purpose:  \n",
    "This function assesses how well the model performs on unseen data by reporting its accuracy and detailed performance metrics, helping identify areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ft4l0vjdLo0j"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def evaluate_model(model, test_loader, label_mapping):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and prints accuracy and classification report.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model to evaluate.\n",
    "        test_loader: DataLoader for the test dataset.\n",
    "        label_mapping: Reverse mapping from model labels to original labels.\n",
    "\n",
    "    Returns:\n",
    "        predictions, true_labels: Lists of predicted and actual labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Move data to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Reverse map predictions and true labels to original labels\n",
    "    reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "    mapped_predictions = [reverse_label_mapping[p] for p in predictions]\n",
    "    mapped_true_labels = [reverse_label_mapping[t] for t in true_labels]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(mapped_true_labels, mapped_predictions) * 100\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"Accuracy: {acc:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(mapped_true_labels, mapped_predictions, digits=4))\n",
    "\n",
    "    return mapped_predictions, mapped_true_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will evaluate the trained model on the test dataset, using the specified label mapping, and print out the accuracy and detailed classification report. The evaluation involves the following steps:\n",
    "\n",
    "1. **Label Mapping**: It uses the `label_mapping` dictionary to map the internal model labels to the original labels of the dataset. \n",
    "   \n",
    "2. **Evaluate Function**: The `evaluate_model` function will iterate over the test dataset using the `test_loader`, making predictions on the input text, and comparing these predictions to the actual labels.\n",
    "\n",
    "3. **Metrics**: After evaluating the model's predictions, the function will output:\n",
    "   - **Accuracy**: The percentage of correct predictions.\n",
    "   - **Classification Report**: A detailed performance summary that includes precision, recall, and F1 score for each label.\n",
    "\n",
    "Once the model is evaluated, you’ll get an output showing its accuracy and the classification report, which will help you understand its performance on each class (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSfY4Efwf9-9",
    "outputId": "04293000-5c4c-4bb1-a91c-8868f5ffe018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.77%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8106    0.5942    0.6857      2846\n",
      "           1     0.8361    0.8975    0.8657      7817\n",
      "           3     0.8126    0.7550    0.7827      5249\n",
      "           4     0.7978    0.8829    0.8382      5228\n",
      "\n",
      "    accuracy                         0.8177     21140\n",
      "   macro avg     0.8143    0.7824    0.7931     21140\n",
      "weighted avg     0.8174    0.8177    0.8141     21140\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  ...],\n",
       " [4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  ...])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "label_mapping = {0: 0, 1: 1, 3: 2, 4: 3}  # Original to remapped labels\n",
    "evaluate_model(model, test_loader, label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model and Tokenizer:\n",
    "\n",
    "1. **`model.save_pretrained(\"arabic_sentiment_model\")`**:\n",
    "   - This saves the trained model to the specified directory, `arabic_sentiment_model`.\n",
    "   - The model's state dictionary (which contains the weights learned during training) is saved, allowing you to reload the model later for inference or further fine-tuning.\n",
    "\n",
    "2. **`tokenizer.save_pretrained(\"arabic_sentiment_model\")`**:\n",
    "   - This saves the tokenizer used for processing input text during training to the same directory.\n",
    "   - The tokenizer is essential because it ensures that the same text preprocessing steps (tokenization, padding, truncation) are applied when you load the model for inference later.\n",
    "   - The saved tokenizer includes the vocabulary and configuration needed to correctly tokenize the text, ensuring consistent handling of input during both training and inference.\n",
    "\n",
    "#### Directory Structure:\n",
    "The saved model and tokenizer will be stored in the `arabic_sentiment_model` directory, which will contain:\n",
    "- **`config.json`**: The configuration of the model (such as model architecture and settings).\n",
    "- **`tokenizer_config.json`**: The tokenizer configuration.\n",
    "- **`vocab.txt`**: The vocabulary used by the tokenizer.\n",
    "\n",
    "This makes it easier to deploy or reload the model and tokenizer for inference or continued training at a later stage. You can reload the saved model and tokenizer with:\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"arabic_sentiment_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"arabic_sentiment_model\")\n",
    "```\n",
    "\n",
    "This code would restore the model and tokenizer from the saved directory so that they are ready to be used again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbSTi9vxMbkA",
    "outputId": "73b1f15c-36d7-4b8c-dab6-df785009629b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic_sentiment_model/tokenizer_config.json',\n",
       " 'arabic_sentiment_model/special_tokens_map.json',\n",
       " 'arabic_sentiment_model/vocab.txt',\n",
       " 'arabic_sentiment_model/added_tokens.json',\n",
       " 'arabic_sentiment_model/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"arabic_sentiment_model\")\n",
    "tokenizer.save_pretrained(\"arabic_sentiment_model\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
